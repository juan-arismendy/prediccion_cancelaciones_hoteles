{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juanda16/prediccion_cancelaciones_hoteles/blob/main/proyecto_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGsLMg0N4-KL",
        "outputId": "d2d3807b-d307-4441-f12a-afe9b73a572a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: pip\n",
            "mkdir: /Users/juan_arismendy/.kaggle: File exists\n",
            "zsh:1: command not found: kaggle\n",
            "Archive:  hotel-booking.zip\n",
            "replace hotel_booking.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
          ]
        }
      ],
      "source": [
        "## install kagle to download the DB directly\n",
        "## Only run this cell if you want to download the DB directly from kaggle and have the kaggle.json file in the same directory as the notebook\n",
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d mojtaba142/hotel-booking\n",
        "!unzip hotel-booking.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwotoA8Ya8iZ",
        "outputId": "c4f6265c-3946-470c-a898-bd9ada84852e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset 'hotel_booking.csv' loaded into DataFrame successfully!\n",
            "\n",
            "Initial DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 119390 entries, 0 to 119389\n",
            "Data columns (total 36 columns):\n",
            " #   Column                          Non-Null Count   Dtype  \n",
            "---  ------                          --------------   -----  \n",
            " 0   hotel                           119390 non-null  object \n",
            " 1   is_canceled                     119390 non-null  int64  \n",
            " 2   lead_time                       119390 non-null  int64  \n",
            " 3   arrival_date_year               119390 non-null  int64  \n",
            " 4   arrival_date_month              119390 non-null  object \n",
            " 5   arrival_date_week_number        119390 non-null  int64  \n",
            " 6   arrival_date_day_of_month       119390 non-null  int64  \n",
            " 7   stays_in_weekend_nights         119390 non-null  int64  \n",
            " 8   stays_in_week_nights            119390 non-null  int64  \n",
            " 9   adults                          119390 non-null  int64  \n",
            " 10  children                        119386 non-null  float64\n",
            " 11  babies                          119390 non-null  int64  \n",
            " 12  meal                            119390 non-null  object \n",
            " 13  country                         118902 non-null  object \n",
            " 14  market_segment                  119390 non-null  object \n",
            " 15  distribution_channel            119390 non-null  object \n",
            " 16  is_repeated_guest               119390 non-null  int64  \n",
            " 17  previous_cancellations          119390 non-null  int64  \n",
            " 18  previous_bookings_not_canceled  119390 non-null  int64  \n",
            " 19  reserved_room_type              119390 non-null  object \n",
            " 20  assigned_room_type              119390 non-null  object \n",
            " 21  booking_changes                 119390 non-null  int64  \n",
            " 22  deposit_type                    119390 non-null  object \n",
            " 23  agent                           103050 non-null  float64\n",
            " 24  company                         6797 non-null    float64\n",
            " 25  days_in_waiting_list            119390 non-null  int64  \n",
            " 26  customer_type                   119390 non-null  object \n",
            " 27  adr                             119390 non-null  float64\n",
            " 28  required_car_parking_spaces     119390 non-null  int64  \n",
            " 29  total_of_special_requests       119390 non-null  int64  \n",
            " 30  reservation_status              119390 non-null  object \n",
            " 31  reservation_status_date         119390 non-null  object \n",
            " 32  name                            119390 non-null  object \n",
            " 33  email                           119390 non-null  object \n",
            " 34  phone-number                    119390 non-null  object \n",
            " 35  credit_card                     119390 non-null  object \n",
            "dtypes: float64(4), int64(16), object(16)\n",
            "memory usage: 32.8+ MB\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## set the DB in Memory as 'df' and show the DB info\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "#  --- 1. Load Data into Pandas DataFrame ---\n",
        "file_name = 'hotel_booking.csv'\n",
        "try:\n",
        "    fetched_df = pd.read_csv(file_name)\n",
        "    print(f\"\\nDataset '{os.path.basename(file_name)}' loaded into DataFrame successfully!\\n\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: CSV file not found at {file_name} after unzipping. Please check the unzipped contents.\")\n",
        "    exit()\n",
        "\n",
        "# Display initial info to understand data types and non-null counts\n",
        "print(\"Initial DataFrame Info:\")\n",
        "fetched_df.info()\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ULqXh8OT5HO",
        "outputId": "f1b376dd-4815-4c6e-83c8-74818cece766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reducing dataset size to 10.00% of original rows.\n",
            "New dataset size: 11939 rows.\n",
            "\n",
            "DataFrame after potential size reduction:\n",
            "               hotel  lead_time  arrival_date_year arrival_date_month  \\\n",
            "30946   Resort Hotel        203               2016           December   \n",
            "40207     City Hotel         82               2015               July   \n",
            "103708    City Hotel         25               2016           December   \n",
            "85144     City Hotel          1               2016              March   \n",
            "109991    City Hotel         70               2017              April   \n",
            "\n",
            "        arrival_date_week_number  arrival_date_day_of_month  \\\n",
            "30946                         49                          2   \n",
            "40207                         29                         16   \n",
            "103708                        53                         27   \n",
            "85144                         11                          9   \n",
            "109991                        16                         16   \n",
            "\n",
            "        stays_in_weekend_nights  stays_in_week_nights  adults  children  ...  \\\n",
            "30946                         2                     5       2       0.0  ...   \n",
            "40207                         0                     3       2       0.0  ...   \n",
            "103708                        0                     3       3       0.0  ...   \n",
            "85144                         0                     1       1       0.0  ...   \n",
            "109991                        2                     2       2       0.0  ...   \n",
            "\n",
            "          customer_type    adr required_car_parking_spaces  \\\n",
            "30946         Transient   66.8                           0   \n",
            "40207         Transient   76.5                           0   \n",
            "103708  Transient-Party   60.0                           0   \n",
            "85144   Transient-Party   95.0                           0   \n",
            "109991        Transient  108.0                           0   \n",
            "\n",
            "       total_of_special_requests reservation_status  reservation_status_date  \\\n",
            "30946                          0          Check-Out               2016-12-09   \n",
            "40207                          0           Canceled               2015-07-16   \n",
            "103708                         1          Check-Out               2016-12-30   \n",
            "85144                          0          Check-Out               2016-03-10   \n",
            "109991                         0          Check-Out               2017-04-20   \n",
            "\n",
            "                        name                          email  phone-number  \\\n",
            "30946      Christine Sanders  Sanders_Christine@outlook.com  530-703-7317   \n",
            "40207       Mr. Jason Cooper            Mr._C63@verizon.com  897-631-4121   \n",
            "103708  Jonathan Goodwin Jr.        JonathanJr.@comcast.net  341-353-2913   \n",
            "85144        Brandon Harrell   Brandon_Harrell12@yandex.com  360-134-7604   \n",
            "109991            Tyler Todd                TTodd@gmail.com  698-685-4845   \n",
            "\n",
            "             credit_card  \n",
            "30946   ************5253  \n",
            "40207   ************3136  \n",
            "103708  ************7754  \n",
            "85144   ************5214  \n",
            "109991  ************4766  \n",
            "\n",
            "[5 rows x 35 columns]\n",
            "Target variable distribution after potential size reduction:\n",
            "is_canceled\n",
            "0    0.622163\n",
            "1    0.377837\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Numerical features identified: ['lead_time', 'arrival_date_year', 'arrival_date_week_number', 'arrival_date_day_of_month', 'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children', 'babies', 'is_repeated_guest', 'previous_cancellations', 'previous_bookings_not_canceled', 'booking_changes', 'agent', 'company', 'days_in_waiting_list', 'adr', 'required_car_parking_spaces', 'total_of_special_requests']\n",
            "Categorical features identified: ['hotel', 'arrival_date_month', 'meal', 'country', 'market_segment', 'distribution_channel', 'reserved_room_type', 'assigned_room_type', 'deposit_type', 'customer_type', 'reservation_status', 'reservation_status_date', 'name', 'email', 'phone-number', 'credit_card']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Reduce the dataset size by sampling a percentage of rows\n",
        "# This is needed to reduce the dataset size to avoid memory issues\n",
        "percentage_to_keep = 0.1  # @param {type:\"slider\", min:0.01, max:1.0, step:0.01}\n",
        "\n",
        "if percentage_to_keep < 1.0:\n",
        "    print(f\"Reducing dataset size to {percentage_to_keep*100:.2f}% of original rows.\")\n",
        "    df = fetched_df.sample(frac=percentage_to_keep, random_state=42) # Use random_state for reproducibility\n",
        "    print(f\"New dataset size: {df.shape[0]} rows.\")\n",
        "\n",
        "# Separate features (X) and target (y) from the potentially reduced DataFrame\n",
        "X = df.drop('is_canceled', axis=1)\n",
        "y = df['is_canceled']\n",
        "\n",
        "print(\"\\nDataFrame after potential size reduction:\")\n",
        "print(X.head())\n",
        "print(f\"Target variable distribution after potential size reduction:\\n{y.value_counts(normalize=True)}\\n\")\n",
        "\n",
        "# Re-identify numerical and categorical features based on the potentially reduced DataFrame if necessary,\n",
        "# though column types should remain consistent.\n",
        "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "print(f\"Numerical features identified: {numerical_features}\")\n",
        "print(f\"Categorical features identified: {categorical_features}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2aTZnibZszg",
        "outputId": "c891edd5-8af3-42a5-85bc-eb697b64888e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed 26 rows with 0 total guests.\n",
            "\n",
            "DataFrame after initial cleaning and feature engineering:\n",
            "               hotel  lead_time  arrival_date_year arrival_date_month  \\\n",
            "30946   Resort Hotel        203               2016           December   \n",
            "40207     City Hotel         82               2015               July   \n",
            "103708    City Hotel         25               2016           December   \n",
            "85144     City Hotel          1               2016              March   \n",
            "109991    City Hotel         70               2017              April   \n",
            "\n",
            "        arrival_date_week_number  arrival_date_day_of_month  \\\n",
            "30946                         49                          2   \n",
            "40207                         29                         16   \n",
            "103708                        53                         27   \n",
            "85144                         11                          9   \n",
            "109991                        16                         16   \n",
            "\n",
            "        stays_in_weekend_nights  stays_in_week_nights  adults  children  ...  \\\n",
            "30946                         2                     5       2       0.0  ...   \n",
            "40207                         0                     3       2       0.0  ...   \n",
            "103708                        0                     3       3       0.0  ...   \n",
            "85144                         0                     1       1       0.0  ...   \n",
            "109991                        2                     2       2       0.0  ...   \n",
            "\n",
            "        company days_in_waiting_list    customer_type    adr  \\\n",
            "30946       0.0                    0        Transient   66.8   \n",
            "40207       0.0                    0        Transient   76.5   \n",
            "103708      0.0                    0  Transient-Party   60.0   \n",
            "85144       0.0                    0  Transient-Party   95.0   \n",
            "109991      0.0                    0        Transient  108.0   \n",
            "\n",
            "       required_car_parking_spaces  total_of_special_requests  \\\n",
            "30946                            0                          0   \n",
            "40207                            0                          0   \n",
            "103708                           0                          1   \n",
            "85144                            0                          0   \n",
            "109991                           0                          0   \n",
            "\n",
            "                        name                          email  phone-number  \\\n",
            "30946      Christine Sanders  Sanders_Christine@outlook.com  530-703-7317   \n",
            "40207       Mr. Jason Cooper            Mr._C63@verizon.com  897-631-4121   \n",
            "103708  Jonathan Goodwin Jr.        JonathanJr.@comcast.net  341-353-2913   \n",
            "85144        Brandon Harrell   Brandon_Harrell12@yandex.com  360-134-7604   \n",
            "109991            Tyler Todd                TTodd@gmail.com  698-685-4845   \n",
            "\n",
            "             credit_card  \n",
            "30946   ************5253  \n",
            "40207   ************3136  \n",
            "103708  ************7754  \n",
            "85144   ************5214  \n",
            "109991  ************4766  \n",
            "\n",
            "[5 rows x 33 columns]\n",
            "Target variable distribution:\n",
            "is_canceled\n",
            "0    0.62159\n",
            "1    0.37841\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Numerical features identified: ['lead_time', 'arrival_date_year', 'arrival_date_week_number', 'arrival_date_day_of_month', 'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children', 'babies', 'is_repeated_guest', 'previous_cancellations', 'previous_bookings_not_canceled', 'booking_changes', 'agent', 'company', 'days_in_waiting_list', 'adr', 'required_car_parking_spaces', 'total_of_special_requests']\n",
            "Categorical features identified: ['hotel', 'arrival_date_month', 'meal', 'country', 'market_segment', 'distribution_channel', 'reserved_room_type', 'assigned_room_type', 'deposit_type', 'customer_type', 'name', 'email', 'phone-number', 'credit_card']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Initial Data Cleaning & Feature Engineering ---\n",
        "\n",
        "# Handle missing values\n",
        "df['children'] = df['children'].fillna(0)\n",
        "df['agent'] = df['agent'].fillna(0)\n",
        "df['company'] = df['company'].fillna(0)\n",
        "\n",
        "\n",
        "# Handle 'adr' (Average Daily Rate): Remove rows with 0 or negative ADR, as they are often data errors.\n",
        "df = df[df['adr'] >= 0]\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True) # Replace any inf with NaN if they exist\n",
        "df.dropna(subset=['adr'], inplace=True) # Drop rows where adr might have become NaN after inf replacement\n",
        "\n",
        "# Remove rows where adults, children, and babies are all zero.\n",
        "initial_rows = df.shape[0]\n",
        "df = df[df['adults'] + df['children'] + df['babies'] > 0]\n",
        "print(f\"Removed {initial_rows - df.shape[0]} rows with 0 total guests.\\n\")\n",
        "\n",
        "# # Feature Engineering\n",
        "# df['total_nights_stay'] = df['stays_in_weekend_nights'] + df['stays_in_week_nights']\n",
        "# df['total_guests'] = df['adults'] + df['children'] + df['babies']\n",
        "# df['room_type_changed'] = (df['assigned_room_type'] != df['reserved_room_type']).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "# # Drop features that are leakage or less relevant after engineering\n",
        "# df = df.drop(columns=['reservation_status', 'reservation_status_date',\n",
        "#                       'stays_in_weekend_nights', 'stays_in_week_nights',\n",
        "#                       'adults', 'children', 'babies',\n",
        "#                       'assigned_room_type', 'reserved_room_type'])\n",
        "\n",
        "# Remove 'reservation_status' and 'reservation_status_date' as they directly indicate cancellation,\n",
        "# and are leakage if used for predicting 'is_canceled'.\n",
        "df = df.drop(columns=['reservation_status', 'reservation_status_date'])\n",
        "\n",
        "# Convert month names to numbers for consistency if needed later, or handle as categorical\n",
        "# For now, let's keep 'arrival_date_month' as categorical since OneHotEncoder will handle it.\n",
        "# If you want numerical month, you would do:\n",
        "# month_map = {'January':1, ..., 'December':12}\n",
        "# df['arrival_date_month'] = df['arrival_date_month'].map(month_map)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('is_canceled', axis=1)\n",
        "y = df['is_canceled']\n",
        "\n",
        "\n",
        "print(\"DataFrame after initial cleaning and feature engineering:\")\n",
        "print(X.head())\n",
        "print(f\"Target variable distribution:\\n{y.value_counts(normalize=True)}\\n\")\n",
        "\n",
        "# Identify numerical and categorical features for preprocessing pipelines\n",
        "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "print(f\"Numerical features identified: {numerical_features}\")\n",
        "print(f\"Categorical features identified: {categorical_features}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4w0Ydg9cWYL",
        "outputId": "303656ec-7bfa-49f0-dc43-ea8daf1607e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing pipeline defined successfully.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Define Preprocessing Steps using Pipelines ---\n",
        "\n",
        "# Numerical features will be scaled\n",
        "numerical_transformer = StandardScaler()\n",
        "\n",
        "# Categorical features will be One-Hot Encoded\n",
        "# handle_unknown='ignore' will set unknown categories to zeros, preventing errors during prediction\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "# This applies different transformers to different columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough' # Keep columns not specified (e.g., if there were others)\n",
        ")\n",
        "\n",
        "print(\"Preprocessing pipeline defined successfully.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mNcoVwTplWE0"
      },
      "outputs": [],
      "source": [
        "# --- 4. Model Training and Evaluation Setup ---\n",
        "# Stratified K-Fold Cross-Validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Function to evaluate a model\n",
        "def evaluate_model(model, X_data, y_data, model_name=\"Model\"):\n",
        "    f1_scores, auc_roc_scores = [], []\n",
        "    accuracy_scores, precision_scores, recall_scores = [], [], []\n",
        "\n",
        "    print(f\"\\n--- Evaluating {model_name} ---\")\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_data, y_data)):\n",
        "        X_train, X_val = X_data.iloc[train_idx], X_data.iloc[val_idx]\n",
        "        y_train, y_val = y_data.iloc[train_idx], y_data.iloc[val_idx]\n",
        "\n",
        "        # Fit the pipeline (includes preprocessing and SMOTE on training data)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict(X_val)\n",
        "        y_proba = model.predict_proba(X_val)[:, 1] # Probability for the positive class\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy_scores.append(accuracy_score(y_val, y_pred))\n",
        "        precision_scores.append(precision_score(y_val, y_pred))\n",
        "        recall_scores.append(recall_score(y_val, y_pred))\n",
        "        f1_scores.append(f1_score(y_val, y_pred))\n",
        "        auc_roc_scores.append(roc_auc_score(y_val, y_proba))\n",
        "\n",
        "        print(f\"  Fold {fold+1}: F1 = {f1_scores[-1]:.3f}, AUC = {auc_roc_scores[-1]:.3f}\")\n",
        "\n",
        "    print(f\"\\n{model_name} - Average Results:\")\n",
        "    print(f\"  Accuracy: {np.mean(accuracy_scores):.3f} +/- {np.std(accuracy_scores)*2:.3f} (95% CI)\")\n",
        "    print(f\"  Precision: {np.mean(precision_scores):.3f} +/- {np.std(precision_scores)*2:.3f} (95% CI)\")\n",
        "    print(f\"  Recall: {np.mean(recall_scores):.3f} +/- {np.std(recall_scores)*2:.3f} (95% CI)\")\n",
        "    print(f\"  F1-Score: {np.mean(f1_scores):.3f} +/- {np.std(f1_scores)*2:.3f} (95% CI)\")\n",
        "    print(f\"  AUC-ROC: {np.mean(auc_roc_scores):.3f} +/- {np.std(auc_roc_scores)*2:.3f} (95% CI)\")\n",
        "\n",
        "    return {\n",
        "        'F1-Score': np.mean(f1_scores),\n",
        "        'AUC-ROC': np.mean(auc_roc_scores),\n",
        "        'F1-CI': np.std(f1_scores) * 2,\n",
        "        'AUC-CI': np.std(auc_roc_scores) * 2\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Htv_R8uCco4v",
        "outputId": "cd229115-82c6-4074-e4da-2c68f8568a21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Model Training and Evaluation ---\n",
            "\n",
            "--- Model 1: Logistic Regression ---\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python(71000) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71001) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71002) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71003) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71005) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71006) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71007) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71008) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71009) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71010) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71011) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71012) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for Logistic Regression: {'classifier__C': 1}\n",
            "\n",
            "--- Evaluating Optimized Logistic Regression ---\n",
            "  Fold 1: F1 = 0.748, AUC = 0.884\n",
            "  Fold 2: F1 = 0.779, AUC = 0.909\n",
            "  Fold 3: F1 = 0.772, AUC = 0.902\n",
            "  Fold 4: F1 = 0.764, AUC = 0.901\n",
            "  Fold 5: F1 = 0.778, AUC = 0.904\n",
            "\n",
            "Optimized Logistic Regression - Average Results:\n",
            "  Accuracy: 0.825 +/- 0.018 (95% CI)\n",
            "  Precision: 0.769 +/- 0.026 (95% CI)\n",
            "  Recall: 0.768 +/- 0.024 (95% CI)\n",
            "  F1-Score: 0.768 +/- 0.023 (95% CI)\n",
            "  AUC-ROC: 0.900 +/- 0.017 (95% CI)\n"
          ]
        }
      ],
      "source": [
        "# --- 5. Model Implementations ---\n",
        "\n",
        "# All models will be part of an ImbPipeline that includes:\n",
        "# 1. Preprocessing (scaling numerical, one-hot encoding categorical)\n",
        "# 2. SMOTE (applied only to training data in each fold)\n",
        "# 3. The classifier itself\n",
        "\n",
        "print(\"\\n--- Starting Model Training and Evaluation ---\")\n",
        "\n",
        "### Model 1: Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "print(\"\\n--- Model 1: Logistic Regression ---\")\n",
        "\n",
        "# Logistic Regression model implementation and evaluation\n",
        "\n",
        "class DenseTransformer():\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X.toarray()\n",
        "\n",
        "pipeline_lr = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', LogisticRegression(random_state=42, solver='liblinear', penalty='l1', max_iter=1000))\n",
        "])\n",
        "\n",
        "param_grid_lr = {\n",
        "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100] # C values to test\n",
        "}\n",
        "grid_search_lr = GridSearchCV(pipeline_lr, param_grid_lr, cv=cv, scoring='f1', n_jobs=-1, verbose=1)\n",
        "grid_search_lr.fit(X, y)\n",
        "\n",
        "print(f\"Best parameters for Logistic Regression: {grid_search_lr.best_params_}\")\n",
        "\n",
        "best_lr_model = grid_search_lr.best_estimator_\n",
        "\n",
        "results_lr = evaluate_model(best_lr_model, X, y, \"Optimized Logistic Regression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Model Training and Evaluation ---\n",
            "\n",
            "--- Model 1: Logistic Regression ---\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python(71000) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71001) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71002) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71003) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71005) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71006) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71007) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71008) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71009) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71010) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71011) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71012) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for Logistic Regression: {'classifier__C': 1}\n",
            "\n",
            "--- Evaluating Optimized Logistic Regression ---\n",
            "  Fold 1: F1 = 0.748, AUC = 0.884\n",
            "  Fold 2: F1 = 0.779, AUC = 0.909\n",
            "  Fold 3: F1 = 0.772, AUC = 0.902\n",
            "  Fold 4: F1 = 0.764, AUC = 0.901\n",
            "  Fold 5: F1 = 0.778, AUC = 0.904\n",
            "\n",
            "Optimized Logistic Regression - Average Results:\n",
            "  Accuracy: 0.825 +/- 0.018 (95% CI)\n",
            "  Precision: 0.769 +/- 0.026 (95% CI)\n",
            "  Recall: 0.768 +/- 0.024 (95% CI)\n",
            "  F1-Score: 0.768 +/- 0.023 (95% CI)\n",
            "  AUC-ROC: 0.900 +/- 0.017 (95% CI)\n"
          ]
        }
      ],
      "source": [
        "# --- . Model Implementations ---\n",
        "\n",
        "# All models will be part of an ImbPipeline that includes:\n",
        "# 1. Preprocessing (scaling numerical, one-hot encoding categorical)\n",
        "# 2. SMOTE (applied only to training data in each fold)\n",
        "# 3. The classifier itself\n",
        "\n",
        "print(\"\\n--- Starting Model Training and Evaluation ---\")\n",
        "\n",
        "### Model 1: Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "print(\"\\n--- Model 1: Logistic Regression ---\")\n",
        "\n",
        "# Logistic Regression model implementation and evaluation\n",
        "\n",
        "class DenseTransformer():\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X.toarray()\n",
        "\n",
        "pipeline_lr = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', LogisticRegression(random_state=42, solver='liblinear', penalty='l1', max_iter=1000))\n",
        "])\n",
        "\n",
        "param_grid_lr = {\n",
        "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100] # C values to test\n",
        "}\n",
        "grid_search_lr = GridSearchCV(pipeline_lr, param_grid_lr, cv=cv, scoring='f1', n_jobs=-1, verbose=1)\n",
        "grid_search_lr.fit(X, y)\n",
        "\n",
        "print(f\"Best parameters for Logistic Regression: {grid_search_lr.best_params_}\")\n",
        "\n",
        "best_lr_model = grid_search_lr.best_estimator_\n",
        "\n",
        "results_lr = evaluate_model(best_lr_model, X, y, \"Optimized Logistic Regression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Model Training and Evaluation ---\n",
            "\n",
            "--- Model 1: Logistic Regression ---\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python(71000) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71001) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71002) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71003) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71005) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71006) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71007) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71008) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71009) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71010) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71011) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(71012) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for Logistic Regression: {'classifier__C': 1}\n",
            "\n",
            "--- Evaluating Optimized Logistic Regression ---\n",
            "  Fold 1: F1 = 0.748, AUC = 0.884\n",
            "  Fold 2: F1 = 0.779, AUC = 0.909\n",
            "  Fold 3: F1 = 0.772, AUC = 0.902\n",
            "  Fold 4: F1 = 0.764, AUC = 0.901\n",
            "  Fold 5: F1 = 0.778, AUC = 0.904\n",
            "\n",
            "Optimized Logistic Regression - Average Results:\n",
            "  Accuracy: 0.825 +/- 0.018 (95% CI)\n",
            "  Precision: 0.769 +/- 0.026 (95% CI)\n",
            "  Recall: 0.768 +/- 0.024 (95% CI)\n",
            "  F1-Score: 0.768 +/- 0.023 (95% CI)\n",
            "  AUC-ROC: 0.900 +/- 0.017 (95% CI)\n"
          ]
        }
      ],
      "source": [
        "# --- . Model Implementations ---\n",
        "\n",
        "# All models will be part of an ImbPipeline that includes:\n",
        "# 1. Preprocessing (scaling numerical, one-hot encoding categorical)\n",
        "# 2. SMOTE (applied only to training data in each fold)\n",
        "# 3. The classifier itself\n",
        "\n",
        "print(\"\\n--- Starting Model Training and Evaluation ---\")\n",
        "\n",
        "### Model 1: Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "print(\"\\n--- Model 1: Logistic Regression ---\")\n",
        "\n",
        "# Logistic Regression model implementation and evaluation\n",
        "\n",
        "class DenseTransformer():\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X.toarray()\n",
        "\n",
        "pipeline_lr = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', LogisticRegression(random_state=42, solver='liblinear', penalty='l1', max_iter=1000))\n",
        "])\n",
        "\n",
        "param_grid_lr = {\n",
        "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100] # C values to test\n",
        "}\n",
        "grid_search_lr = GridSearchCV(pipeline_lr, param_grid_lr, cv=cv, scoring='f1', n_jobs=-1, verbose=1)\n",
        "grid_search_lr.fit(X, y)\n",
        "\n",
        "print(f\"Best parameters for Logistic Regression: {grid_search_lr.best_params_}\")\n",
        "\n",
        "best_lr_model = grid_search_lr.best_estimator_\n",
        "\n",
        "results_lr = evaluate_model(best_lr_model, X, y, \"Optimized Logistic Regression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HEL9MrxlkHn",
        "outputId": "645d0a97-3014-4f81-c0fc-700e030063a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Model 2: K-Nearest Neighbors (KNN) ---\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters for KNN: {'classifier__n_neighbors': 11, 'classifier__p': 1, 'classifier__weights': 'distance'}\n",
            "\n",
            "--- Evaluating Optimized KNN ---\n",
            "  Fold 1: F1 = 0.726, AUC = 0.873\n",
            "  Fold 2: F1 = 0.764, AUC = 0.892\n",
            "  Fold 3: F1 = 0.739, AUC = 0.876\n",
            "  Fold 4: F1 = 0.726, AUC = 0.874\n",
            "  Fold 5: F1 = 0.763, AUC = 0.886\n",
            "\n",
            "Optimized KNN - Average Results:\n",
            "  Accuracy: 0.814 +/- 0.025 (95% CI)\n",
            "  Precision: 0.779 +/- 0.038 (95% CI)\n",
            "  Recall: 0.712 +/- 0.033 (95% CI)\n",
            "  F1-Score: 0.744 +/- 0.033 (95% CI)\n",
            "  AUC-ROC: 0.880 +/- 0.015 (95% CI)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "### Model 2: K-Nearest Neighbors (KNN)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "print(\"\\n--- Model 2: K-Nearest Neighbors (KNN) ---\")\n",
        "pipeline_knn = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "param_grid_knn = {\n",
        "    'classifier__n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'classifier__weights': ['uniform', 'distance'],\n",
        "    'classifier__p': [1, 2] # 1 for Manhattan distance, 2 for Euclidean distance\n",
        "}\n",
        "\n",
        "grid_search_knn = GridSearchCV(pipeline_knn, param_grid_knn, cv=cv, scoring='f1', n_jobs=-1, verbose=1)\n",
        "grid_search_knn.fit(X, y)\n",
        "\n",
        "print(f\"Best parameters for KNN: {grid_search_knn.best_params_}\")\n",
        "best_knn_model = grid_search_knn.best_estimator_\n",
        "results_knn = evaluate_model(best_knn_model, X, y, \"Optimized KNN\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fb67e2f",
        "outputId": "e3c76e90-ab89-4aed-c53e-4d5dc1113fe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Model 3: Random Forest ---\n",
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for Random Forest: {'classifier__max_depth': 30, 'classifier__max_features': 0.5, 'classifier__min_samples_leaf': 2, 'classifier__n_estimators': 100}\n",
            "\n",
            "--- Evaluating Optimized Random Forest ---\n",
            "  Fold 1: F1 = 0.802, AUC = 0.926\n",
            "  Fold 2: F1 = 0.820, AUC = 0.942\n",
            "  Fold 3: F1 = 0.813, AUC = 0.931\n",
            "  Fold 4: F1 = 0.812, AUC = 0.926\n",
            "  Fold 5: F1 = 0.821, AUC = 0.941\n",
            "\n",
            "Optimized Random Forest - Average Results:\n",
            "  Accuracy: 0.863 +/- 0.010 (95% CI)\n",
            "  Precision: 0.838 +/- 0.019 (95% CI)\n",
            "  Recall: 0.791 +/- 0.016 (95% CI)\n",
            "  F1-Score: 0.814 +/- 0.014 (95% CI)\n",
            "  AUC-ROC: 0.933 +/- 0.014 (95% CI)\n"
          ]
        }
      ],
      "source": [
        "### Model 3: Random Forest (Ensemble of Decision Trees)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "print(\"\\n--- Model 3: Random Forest ---\")\n",
        "pipeline_rf = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid_rf = {\n",
        "    'classifier__n_estimators': [100, 200, 300],\n",
        "    'classifier__max_depth': [10, 20, 30, None],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4],\n",
        "    'classifier__max_features': ['sqrt','log2',0.5],\n",
        "}\n",
        "\n",
        "# Note: Make sure X and y are defined and represent your features and target\n",
        "# (These should be defined in previous data loading/cleaning steps)\n",
        "grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=cv, scoring='f1', n_jobs=-1, verbose=1)\n",
        "grid_search_rf.fit(X, y) # Fit on the full data after cleaning and feature engineering\n",
        "\n",
        "print(f\"Best parameters for Random Forest: {grid_search_rf.best_params_}\")\n",
        "best_rf_model = grid_search_rf.best_estimator_\n",
        "# Evaluate on the full data for final results\n",
        "results_rf = evaluate_model(best_rf_model, X, y, \"Optimized Random Forest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri-Hm_e0WAzL",
        "outputId": "1bb7b13f-d44d-4d5a-9d3b-b50fbc1c48d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Model 4: Artificial Neural Network (MLP) ---\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python(63644) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(63645) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(63646) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(63647) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(63648) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(63649) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(63650) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(63651) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(63652) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(63653) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(63654) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "Python(63656) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for MLP: {'classifier__activation': 'relu', 'classifier__alpha': 0.01, 'classifier__hidden_layer_sizes': (50,), 'classifier__learning_rate': 'constant', 'classifier__solver': 'adam'}\n",
            "\n",
            "--- Evaluating Optimized MLP ---\n",
            "  Fold 1: F1 = 0.711, AUC = 0.873\n",
            "  Fold 2: F1 = 0.737, AUC = 0.904\n",
            "  Fold 3: F1 = 0.743, AUC = 0.892\n",
            "  Fold 4: F1 = 0.736, AUC = 0.894\n",
            "  Fold 5: F1 = 0.737, AUC = 0.896\n",
            "\n",
            "Optimized MLP - Average Results:\n",
            "  Accuracy: 0.813 +/- 0.014 (95% CI)\n",
            "  Precision: 0.800 +/- 0.040 (95% CI)\n",
            "  Recall: 0.677 +/- 0.044 (95% CI)\n",
            "  F1-Score: 0.733 +/- 0.023 (95% CI)\n",
            "  AUC-ROC: 0.892 +/- 0.021 (95% CI)\n"
          ]
        }
      ],
      "source": [
        "### Model 4: Artificial Neural Network (ANN) - Multi-layer Perceptron (MLP)\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "print(\"\\n--- Model 4: Artificial Neural Network (MLP) ---\")\n",
        "pipeline_mlp = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', MLPClassifier(random_state=42, \n",
        "        max_iter=1000, \n",
        "        early_stopping=True,     # Stop early if no improvement\n",
        "        validation_fraction=0.1, # Use 10% for validation\n",
        "        n_iter_no_change=10,     # Stop if no improvement for 10 iterations\n",
        "        tol=1e-4\n",
        "      \n",
        "      )) # Increased max_iter for convergence\n",
        "])\n",
        "\n",
        "param_grid_mlp = {\n",
        "    'classifier__hidden_layer_sizes': [(50,), (100,),(50,50), (100,50)],\n",
        "    'classifier__activation': ['relu'],\n",
        "    'classifier__solver': ['adam'],\n",
        "    'classifier__alpha': [ 0.0001,0.001, 0.01], # L2 regularization\n",
        "    'classifier__learning_rate': ['constant'] # Adaptive learning rate\n",
        "}\n",
        "\n",
        "grid_search_mlp = GridSearchCV(pipeline_mlp, param_grid_mlp, cv=cv, scoring='f1', n_jobs=-1, verbose=1)\n",
        "grid_search_mlp.fit(X, y)\n",
        "\n",
        "print(f\"Best parameters for MLP: {grid_search_mlp.best_params_}\")\n",
        "best_mlp_model = grid_search_mlp.best_estimator_\n",
        "results_mlp = evaluate_model(best_mlp_model, X, y, \"Optimized MLP\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MgQrsNhkWUSI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Model 5: Support Vector Machine (SVM) ---\n",
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "Best parameters for SVM: {'classifier__C': 10, 'classifier__gamma': 'scale', 'classifier__kernel': 'rbf'}\n",
            "\n",
            "--- Evaluating Optimized SVM ---\n",
            "  Fold 1: F1 = 0.786, AUC = 0.912\n",
            "  Fold 2: F1 = 0.805, AUC = 0.930\n",
            "  Fold 3: F1 = 0.794, AUC = 0.924\n",
            "  Fold 4: F1 = 0.792, AUC = 0.921\n",
            "  Fold 5: F1 = 0.808, AUC = 0.927\n",
            "\n",
            "Optimized SVM - Average Results:\n",
            "  Accuracy: 0.846 +/- 0.013 (95% CI)\n",
            "  Precision: 0.797 +/- 0.022 (95% CI)\n",
            "  Recall: 0.797 +/- 0.016 (95% CI)\n",
            "  F1-Score: 0.797 +/- 0.016 (95% CI)\n",
            "  AUC-ROC: 0.923 +/- 0.013 (95% CI)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "### Model 5: Support Vector Machine (SVM)\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# SVMs can be computationally expensive with larger datasets.\n",
        "# Using a linear kernel or a smaller subset of data might be necessary if it's too slow.\n",
        "# For simplicity, let's use a smaller C range and a linear/rbf kernel.\n",
        "print(\"\\n--- Model 5: Support Vector Machine (SVM) ---\")\n",
        "pipeline_svc = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', SVC(random_state=42, probability=True)) # probability=True needed for predict_proba\n",
        "])\n",
        "\n",
        "param_grid_svc = {\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "    'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
        "    'classifier__gamma': ['scale', 'auto', 0.1, 1]\n",
        "}\n",
        "\n",
        "grid_search_svc = GridSearchCV(pipeline_svc, param_grid_svc, cv=cv, scoring='f1', n_jobs=-1, verbose=1)\n",
        "grid_search_svc.fit(X, y)\n",
        "\n",
        "print(f\"Best parameters for SVM: {grid_search_svc.best_params_}\")\n",
        "best_svc_model = grid_search_svc.best_estimator_\n",
        "results_svc = evaluate_model(best_svc_model, X, y, \"Optimized SVM\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RAqXCPF4WX1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- All Model Evaluations Complete ---\n",
            "\n",
            "--- Summary of Best Model Performance ---\n",
            "Logistic Regression: F1-Score = 0.766 +/- 0.023, AUC-ROC = 0.898 +/- 0.017\n",
            "KNN: F1-Score = 0.744 +/- 0.033, AUC-ROC = 0.880 +/- 0.015\n",
            "Random Forest: F1-Score = 0.814 +/- 0.014, AUC-ROC = 0.933 +/- 0.014\n",
            "MLP: F1-Score = 0.733 +/- 0.023, AUC-ROC = 0.892 +/- 0.021\n",
            "SVM: F1-Score = 0.797 +/- 0.016, AUC-ROC = 0.923 +/- 0.013\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n--- All Model Evaluations Complete ---\")\n",
        "\n",
        "# You can collect all results here to generate your summary table\n",
        "all_results = {\n",
        "    \"Logistic Regression\": results_lr,\n",
        "    \"KNN\": results_knn,\n",
        "    \"Random Forest\": results_rf,\n",
        "    \"MLP\": results_mlp,\n",
        "    \"SVM\": results_svc\n",
        "}\n",
        "\n",
        "print(\"\\n--- Summary of Best Model Performance ---\")\n",
        "for model_name, metrics in all_results.items():\n",
        "    print(f\"{model_name}: F1-Score = {metrics['F1-Score']:.3f} +/- {metrics['F1-CI']:.3f}, AUC-ROC = {metrics['AUC-ROC']:.3f} +/- {metrics['AUC-CI']:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
