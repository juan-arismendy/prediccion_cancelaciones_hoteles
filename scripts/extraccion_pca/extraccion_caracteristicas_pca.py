#!/usr/bin/env python3
"""
Script para extracci√≥n de caracter√≠sticas usando PCA
en los dos mejores modelos (Random Forest y SVM).
Incluye justificaci√≥n del criterio de selecci√≥n y evaluaci√≥n completa.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import PCA
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n de estilo
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 11
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 12

# Colores para las gr√°ficas
COLORS = {
    'primary': '#2E86AB',
    'secondary': '#A23B72', 
    'accent': '#F18F01',
    'success': '#C73E1D',
    'neutral': '#6C757D'
}

def load_and_prepare_data():
    """
    Carga y prepara los datos para el an√°lisis PCA
    """
    print("=== CARGA Y PREPARACI√ìN DE DATOS ===")
    
    # Cargar datos
    df = pd.read_csv('hotel_booking.csv')
    
    # Reducir dataset para eficiencia
    df = df.sample(frac=0.1, random_state=42)
    
    # Limpieza b√°sica
    df['children'] = df['children'].fillna(0)
    df['agent'] = df['agent'].fillna(0)
    df['company'] = df['company'].fillna(0)
    df = df[df['adr'] >= 0]
    df = df[df['adults'] + df['children'] + df['babies'] > 0]
    df = df.drop(columns=['reservation_status', 'reservation_status_date'])
    
    # Separar features y target
    X = df.drop('is_canceled', axis=1)
    y = df['is_canceled']
    
    # Identificar caracter√≠sticas num√©ricas y categ√≥ricas
    numerical_features = X.select_dtypes(include=np.number).columns.tolist()
    categorical_features = X.select_dtypes(include='object').columns.tolist()
    
    print(f"Dataset shape: {X.shape}")
    print(f"Caracter√≠sticas num√©ricas: {len(numerical_features)}")
    print(f"Caracter√≠sticas categ√≥ricas: {len(categorical_features)}")
    print(f"Total de caracter√≠sticas: {X.shape[1]}")
    
    return X, y, numerical_features, categorical_features

def create_preprocessor(numerical_features, categorical_features):
    """
    Crea el preprocesador para las caracter√≠sticas
    """
    numerical_transformer = StandardScaler()
    categorical_transformer = OneHotEncoder(handle_unknown='ignore')
    
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_features),
            ('cat', categorical_transformer, categorical_features)
        ],
        remainder='passthrough'
    )
    
    return preprocessor

def justify_pca_criterion():
    """
    Justifica el criterio de selecci√≥n de componentes principales
    """
    print("\n=== JUSTIFICACI√ìN DEL CRITERIO DE SELECCI√ìN DE COMPONENTES ===")
    print("Criterio elegido: 95% de Varianza Explicada Acumulada")
    print("\nJustificaci√≥n:")
    print("1. CONSERVACI√ìN DE INFORMACI√ìN:")
    print("   - 95% de varianza asegura retener la mayor√≠a de la informaci√≥n")
    print("   - Balance entre reducci√≥n dimensional y preservaci√≥n de datos")
    print("   - Est√°ndar ampliamente aceptado en la literatura")
    
    print("\n2. REDUCCI√ìN DIMENSIONAL EFECTIVA:")
    print("   - Elimina redundancia manteniendo caracter√≠sticas discriminativas")
    print("   - Reduce overfitting al eliminar ruido y correlaciones")
    print("   - Mejora eficiencia computacional")
    
    print("\n3. COMPARACI√ìN CON OTROS CRITERIOS:")
    print("   - Kaiser (eigenvalues > 1): Puede ser muy conservador")
    print("   - Scree plot: Subjetivo y dif√≠cil de automatizar")
    print("   - 99% varianza: Demasiado conservador, poca reducci√≥n")
    print("   - 90% varianza: Podr√≠a perder informaci√≥n importante")
    
    print("\n4. EVIDENCIA EMP√çRICA:")
    print("   - 95% es √≥ptimo para datasets con ruido moderado")
    print("   - Mantiene capacidad predictiva en la mayor√≠a de casos")
    print("   - Permite reducci√≥n significativa sin p√©rdida cr√≠tica")
    
    return "95% Varianza Explicada"

def analyze_pca_components(X, y, preprocessor):
    """
    Analiza los componentes principales y determina el n√∫mero √≥ptimo
    """
    print("\n=== AN√ÅLISIS DE COMPONENTES PRINCIPALES ===")
    
    # Aplicar preprocesamiento
    X_preprocessed = preprocessor.fit_transform(X)
    
    # Aplicar PCA completo para an√°lisis
    pca_full = PCA()
    X_pca_full = pca_full.fit_transform(X_preprocessed)
    
    # Calcular varianza explicada acumulada
    cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)
    
    # Encontrar n√∫mero de componentes para 95% de varianza
    n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1
    
    # Encontrar n√∫mero de componentes para otros umbrales
    n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1
    n_components_99 = np.argmax(cumulative_variance >= 0.99) + 1
    
    print(f"Dimensiones originales: {X_preprocessed.shape[1]}")
    print(f"Componentes para 90% varianza: {n_components_90}")
    print(f"Componentes para 95% varianza: {n_components_95}")
    print(f"Componentes para 99% varianza: {n_components_99}")
    
    # Crear gr√°fica de varianza explicada
    plt.figure(figsize=(12, 8))
    
    # Subplot 1: Varianza explicada por componente
    plt.subplot(2, 2, 1)
    plt.plot(range(1, min(51, len(pca_full.explained_variance_ratio_) + 1)), 
             pca_full.explained_variance_ratio_[:50], 'bo-', linewidth=2, markersize=4)
    plt.xlabel('Componente Principal')
    plt.ylabel('Varianza Explicada')
    plt.title('Varianza Explicada por Componente')
    plt.grid(True, alpha=0.3)
    
    # Subplot 2: Varianza explicada acumulada
    plt.subplot(2, 2, 2)
    plt.plot(range(1, min(51, len(cumulative_variance) + 1)), 
             cumulative_variance[:50], 'ro-', linewidth=2, markersize=4)
    plt.axhline(y=0.90, color='orange', linestyle='--', label='90%')
    plt.axhline(y=0.95, color='green', linestyle='--', label='95%')
    plt.axhline(y=0.99, color='red', linestyle='--', label='99%')
    plt.axvline(x=n_components_95, color='green', linestyle=':', alpha=0.7)
    plt.xlabel('N√∫mero de Componentes')
    plt.ylabel('Varianza Explicada Acumulada')
    plt.title('Varianza Explicada Acumulada')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Subplot 3: Comparaci√≥n de umbrales
    plt.subplot(2, 2, 3)
    thresholds = [90, 95, 99]
    components = [n_components_90, n_components_95, n_components_99]
    reduction = [(X_preprocessed.shape[1] - comp) / X_preprocessed.shape[1] * 100 for comp in components]
    
    bars = plt.bar(range(len(thresholds)), components, color=[COLORS['accent'], COLORS['primary'], COLORS['secondary']])
    plt.xlabel('Umbral de Varianza (%)')
    plt.ylabel('N√∫mero de Componentes')
    plt.title('Componentes por Umbral de Varianza')
    plt.xticks(range(len(thresholds)), [f'{t}%' for t in thresholds])
    plt.grid(True, alpha=0.3, axis='y')
    
    # Agregar valores en las barras
    for bar, comp, red in zip(bars, components, reduction):
        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,
                f'{comp}\n({red:.1f}% red.)', ha='center', va='bottom', fontweight='bold')
    
    # Subplot 4: Primeros componentes principales
    plt.subplot(2, 2, 4)
    first_10_variance = pca_full.explained_variance_ratio_[:10]
    plt.bar(range(1, 11), first_10_variance, color=COLORS['neutral'])
    plt.xlabel('Componente Principal')
    plt.ylabel('Varianza Explicada')
    plt.title('Primeros 10 Componentes Principales')
    plt.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig('pca_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return n_components_95, X_preprocessed.shape[1]

def evaluate_model_with_pca(X, y, preprocessor, model, model_name, n_components):
    """
    Eval√∫a un modelo usando PCA con n√∫mero espec√≠fico de componentes
    """
    print(f"\n=== EVALUACI√ìN CON PCA - {model_name} ===")
    print(f"Usando {n_components} componentes principales")
    
    # Crear pipeline con PCA
    pipeline_pca = ImbPipeline(steps=[
        ('preprocessor', preprocessor),
        ('pca', PCA(n_components=n_components, random_state=42)),
        ('smote', SMOTE(random_state=42)),
        ('classifier', model)
    ])
    
    # Evaluar con validaci√≥n cruzada
    cv_scores = cross_val_score(pipeline_pca, X, y, cv=5, scoring='f1')
    
    # Evaluar en conjunto de test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    pipeline_pca.fit(X_train, y_train)
    y_pred = pipeline_pca.predict(X_test)
    y_proba = pipeline_pca.predict_proba(X_test)[:, 1]
    
    # Calcular m√©tricas
    metrics = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred),
        'AUC-ROC': roc_auc_score(y_test, y_proba)
    }
    
    print(f"Resultados en test set:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.3f}")
    
    print(f"F1-Score CV (5-fold): {cv_scores.mean():.3f} ¬± {cv_scores.std()*2:.3f}")
    
    return metrics, cv_scores

def evaluate_model_original(X, y, preprocessor, model, model_name):
    """
    Eval√∫a un modelo sin PCA (caracter√≠sticas originales)
    """
    print(f"\n=== EVALUACI√ìN SIN PCA - {model_name} ===")
    
    # Crear pipeline sin PCA
    pipeline_original = ImbPipeline(steps=[
        ('preprocessor', preprocessor),
        ('smote', SMOTE(random_state=42)),
        ('classifier', model)
    ])
    
    # Evaluar con validaci√≥n cruzada
    cv_scores = cross_val_score(pipeline_original, X, y, cv=5, scoring='f1')
    
    # Evaluar en conjunto de test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    pipeline_original.fit(X_train, y_train)
    y_pred = pipeline_original.predict(X_test)
    y_proba = pipeline_original.predict_proba(X_test)[:, 1]
    
    # Calcular m√©tricas
    metrics = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred),
        'AUC-ROC': roc_auc_score(y_test, y_proba)
    }
    
    print(f"Resultados en test set:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.3f}")
    
    print(f"F1-Score CV (5-fold): {cv_scores.mean():.3f} ¬± {cv_scores.std()*2:.3f}")
    
    return metrics, cv_scores

def create_pca_comparison_table(results_original, results_pca, n_components, original_features):
    """
    Crea tabla comparativa de resultados con y sin PCA
    """
    print("\n=== TABLA COMPARATIVA DE RESULTADOS PCA ===")
    
    comparison_data = []
    
    for model_name in results_original.keys():
        # Resultados originales
        orig_metrics = results_original[model_name]
        comparison_data.append({
            'Modelo': model_name,
            'M√©todo': 'Original',
            'Caracter√≠sticas': original_features,
            'Accuracy': f"{orig_metrics['Accuracy']:.3f}",
            'Precision': f"{orig_metrics['Precision']:.3f}",
            'Recall': f"{orig_metrics['Recall']:.3f}",
            'F1-Score': f"{orig_metrics['F1-Score']:.3f}",
            'AUC-ROC': f"{orig_metrics['AUC-ROC']:.3f}",
            'Reducci√≥n (%)': '0.0%'
        })
        
        # Resultados con PCA
        pca_metrics = results_pca[model_name]
        reduction = ((original_features - n_components) / original_features) * 100
        
        comparison_data.append({
            'Modelo': model_name,
            'M√©todo': 'PCA',
            'Caracter√≠sticas': n_components,
            'Accuracy': f"{pca_metrics['Accuracy']:.3f}",
            'Precision': f"{pca_metrics['Precision']:.3f}",
            'Recall': f"{pca_metrics['Recall']:.3f}",
            'F1-Score': f"{pca_metrics['F1-Score']:.3f}",
            'AUC-ROC': f"{pca_metrics['AUC-ROC']:.3f}",
            'Reducci√≥n (%)': f"{reduction:.1f}%"
        })
    
    df_comparison = pd.DataFrame(comparison_data)
    
    # Crear tabla formateada
    fig, ax = plt.subplots(figsize=(16, 8))
    ax.axis('tight')
    ax.axis('off')
    
    table = ax.table(cellText=df_comparison.values.tolist(), 
                    colLabels=df_comparison.columns.tolist(), 
                    cellLoc='center',
                    loc='center',
                    bbox=[0, 0, 1, 1])
    
    # Formatear tabla
    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1.2, 1.5)
    
    # Colorear encabezados
    for i in range(len(df_comparison.columns)):
        table[(0, i)].set_facecolor(COLORS['primary'])
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    # Colorear filas alternas
    for i in range(1, len(df_comparison) + 1):
        for j in range(len(df_comparison.columns)):
            if i % 2 == 0:
                table[(i, j)].set_facecolor('#f0f0f0')
    
    plt.title('Comparaci√≥n: Caracter√≠sticas Originales vs PCA', 
              fontsize=16, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.savefig('pca_comparison_table.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return df_comparison

def plot_pca_results(results_original, results_pca, n_components, original_features):
    """
    Crea gr√°ficas de resultados de PCA
    """
    print("\n=== GR√ÅFICAS DE RESULTADOS PCA ===")
    
    models = list(results_original.keys())
    
    # Gr√°fica 1: Reducci√≥n de dimensionalidad
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # Gr√°fica de barras de caracter√≠sticas
    x = np.arange(len(models))
    width = 0.35
    
    original_dims = [original_features] * len(models)
    pca_dims = [n_components] * len(models)
    
    bars1 = ax1.bar(x - width/2, original_dims, width, label='Original', color=COLORS['primary'])
    bars2 = ax1.bar(x + width/2, pca_dims, width, label='PCA', color=COLORS['secondary'])
    
    ax1.set_xlabel('Modelos', fontsize=12, fontweight='bold')
    ax1.set_ylabel('N√∫mero de Caracter√≠sticas', fontsize=12, fontweight='bold')
    ax1.set_title('Reducci√≥n Dimensional con PCA', fontsize=14, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(models, rotation=45, ha='right')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Agregar valores en las barras
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 2,
                    f'{int(height)}', ha='center', va='bottom', fontweight='bold')
    
    # Gr√°fica de porcentaje de reducci√≥n
    reduction_pct = ((original_features - n_components) / original_features) * 100
    reductions = [reduction_pct] * len(models)
    
    bars3 = ax2.bar(models, reductions, color=COLORS['accent'])
    ax2.set_xlabel('Modelos', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Reducci√≥n (%)', fontsize=12, fontweight='bold')
    ax2.set_title('Porcentaje de Reducci√≥n Dimensional', fontsize=14, fontweight='bold')
    ax2.set_xticklabels(models, rotation=45, ha='right')
    ax2.grid(True, alpha=0.3, axis='y')
    
    # Agregar valores en las barras
    for bar, reduction in zip(bars3, reductions):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                f'{reduction:.1f}%', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('pca_reduction_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Gr√°fica 2: Comparaci√≥n de m√©tricas
    metrics = ['F1-Score', 'AUC-ROC']
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    for i, metric in enumerate(metrics):
        ax = axes[i]
        
        # Datos originales y PCA
        orig_values = [results_original[model][metric] for model in models]
        pca_values = [results_pca[model][metric] for model in models]
        
        x = np.arange(len(models))
        width = 0.35
        
        bars1 = ax.bar(x - width/2, orig_values, width, label='Original', color=COLORS['primary'])
        bars2 = ax.bar(x + width/2, pca_values, width, label='PCA', color=COLORS['secondary'])
        
        ax.set_xlabel('Modelos', fontsize=12, fontweight='bold')
        ax.set_ylabel(metric, fontsize=12, fontweight='bold')
        ax.set_title(f'Comparaci√≥n de {metric}', fontsize=14, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # Agregar valores en las barras
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{height:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)
    
    plt.tight_layout()
    plt.savefig('pca_metrics_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

def analyze_pca_components_contribution(X, y, preprocessor, n_components):
    """
    Analiza la contribuci√≥n de los componentes principales
    """
    print(f"\n=== AN√ÅLISIS DE CONTRIBUCI√ìN DE COMPONENTES ===")
    
    # Aplicar preprocesamiento
    X_preprocessed = preprocessor.fit_transform(X)
    
    # Aplicar PCA
    pca = PCA(n_components=n_components, random_state=42)
    X_pca = pca.fit_transform(X_preprocessed)
    
    # Mostrar varianza explicada por cada componente
    print(f"Varianza explicada por cada componente (primeros 10):")
    for i, var_exp in enumerate(pca.explained_variance_ratio_[:10], 1):
        print(f"  PC{i}: {var_exp:.4f} ({var_exp*100:.2f}%)")
    
    print(f"\nVarianza explicada acumulada: {pca.explained_variance_ratio_.sum():.4f} ({pca.explained_variance_ratio_.sum()*100:.2f}%)")
    
    # Gr√°fica de contribuci√≥n de componentes
    plt.figure(figsize=(12, 6))
    
    # Subplot 1: Varianza explicada por componente
    plt.subplot(1, 2, 1)
    components_to_show = min(20, n_components)
    plt.bar(range(1, components_to_show + 1), 
            pca.explained_variance_ratio_[:components_to_show], 
            color=COLORS['primary'])
    plt.xlabel('Componente Principal')
    plt.ylabel('Varianza Explicada')
    plt.title(f'Varianza Explicada por Componente\n(Primeros {components_to_show} componentes)')
    plt.grid(True, alpha=0.3, axis='y')
    
    # Subplot 2: Varianza explicada acumulada
    plt.subplot(1, 2, 2)
    cumulative_var = np.cumsum(pca.explained_variance_ratio_[:components_to_show])
    plt.plot(range(1, components_to_show + 1), cumulative_var, 'o-', 
             color=COLORS['secondary'], linewidth=2, markersize=6)
    plt.axhline(y=0.95, color='red', linestyle='--', label='95% Umbral')
    plt.xlabel('N√∫mero de Componentes')
    plt.ylabel('Varianza Explicada Acumulada')
    plt.title('Varianza Explicada Acumulada')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('pca_components_contribution.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return pca.explained_variance_ratio_

def main():
    """
    Funci√≥n principal que ejecuta todo el an√°lisis PCA
    """
    print("=== AN√ÅLISIS DE EXTRACCI√ìN DE CARACTER√çSTICAS CON PCA ===")
    print("Modelos evaluados: Random Forest y SVM")
    print("M√©todo: An√°lisis de Componentes Principales (PCA)")
    
    # 1. Cargar y preparar datos
    X, y, numerical_features, categorical_features = load_and_prepare_data()
    
    # 2. Crear preprocesador
    preprocessor = create_preprocessor(numerical_features, categorical_features)
    
    # 3. Justificar criterio de selecci√≥n
    criterion = justify_pca_criterion()
    
    # 4. Analizar componentes principales
    n_components, original_features = analyze_pca_components(X, y, preprocessor)
    
    print(f"\nüéØ Componentes seleccionados: {n_components}")
    print(f"üéØ Caracter√≠sticas originales: {original_features}")
    print(f"üéØ Reducci√≥n: {((original_features - n_components) / original_features * 100):.1f}%")
    
    # 5. Definir modelos (los dos mejores)
    models = {
        'Random Forest': RandomForestClassifier(
            random_state=42, 
            n_estimators=100, 
            max_depth=30, 
            max_features='sqrt'
        ),
        'SVM': SVC(
            random_state=42, 
            probability=True, 
            C=10, 
            kernel='rbf', 
            gamma='scale'
        )
    }
    
    # 6. Evaluaci√≥n con caracter√≠sticas originales
    print("\n" + "="*60)
    print("EVALUACI√ìN CON CARACTER√çSTICAS ORIGINALES")
    print("="*60)
    
    results_original = {}
    for model_name, model in models.items():
        metrics, cv_scores = evaluate_model_original(X, y, preprocessor, model, model_name)
        results_original[model_name] = metrics
    
    # 7. Evaluaci√≥n con PCA
    print("\n" + "="*60)
    print("EVALUACI√ìN CON PCA")
    print("="*60)
    
    results_pca = {}
    for model_name, model in models.items():
        metrics, cv_scores = evaluate_model_with_pca(X, y, preprocessor, model, model_name, n_components)
        results_pca[model_name] = metrics
    
    # 8. Crear tabla comparativa
    comparison_table = create_pca_comparison_table(results_original, results_pca, n_components, original_features)
    
    # 9. Crear gr√°ficas
    plot_pca_results(results_original, results_pca, n_components, original_features)
    
    # 10. Analizar contribuci√≥n de componentes
    explained_variance = analyze_pca_components_contribution(X, y, preprocessor, n_components)
    
    # 11. Resumen final
    print("\n" + "="*80)
    print("RESUMEN FINAL DE EXTRACCI√ìN DE CARACTER√çSTICAS PCA")
    print("="*80)
    
    print(f"\nCriterio de selecci√≥n: {criterion}")
    print("Justificaci√≥n: 95% de varianza explicada es el est√°ndar para mantener")
    print("informaci√≥n cr√≠tica mientras se logra reducci√≥n dimensional significativa.")
    
    print(f"\nReducci√≥n dimensional:")
    print(f"  - Caracter√≠sticas originales: {original_features}")
    print(f"  - Componentes principales: {n_components}")
    print(f"  - Reducci√≥n: {((original_features - n_components) / original_features * 100):.1f}%")
    print(f"  - Varianza explicada: {explained_variance.sum():.1%}")
    
    print(f"\nResultados por modelo:")
    for model_name in models.keys():
        orig_f1 = results_original[model_name]['F1-Score']
        pca_f1 = results_pca[model_name]['F1-Score']
        f1_change = ((pca_f1 - orig_f1) / orig_f1) * 100
        
        orig_auc = results_original[model_name]['AUC-ROC']
        pca_auc = results_pca[model_name]['AUC-ROC']
        auc_change = ((pca_auc - orig_auc) / orig_auc) * 100
        
        print(f"\n{model_name}:")
        print(f"  - F1-Score original: {orig_f1:.3f}")
        print(f"  - F1-Score PCA: {pca_f1:.3f}")
        print(f"  - Cambio en F1-Score: {f1_change:+.1f}%")
        print(f"  - AUC-ROC original: {orig_auc:.3f}")
        print(f"  - AUC-ROC PCA: {pca_auc:.3f}")
        print(f"  - Cambio en AUC-ROC: {auc_change:+.1f}%")
    
    print(f"\n‚úÖ An√°lisis PCA completado. Gr√°ficas guardadas en alta resoluci√≥n.")

if __name__ == "__main__":
    main() 