#!/usr/bin/env python3
"""
Script para an谩lisis de caracter铆sticas y reducci贸n de dimensionalidad
en el proyecto de predicci贸n de cancelaciones de hoteles.
Incluye:
- An谩lisis de correlaci贸n
- Capacidad discriminativa de variables
- Selecci贸n de caracter铆sticas
- Evaluaci贸n de t茅cnicas de reducci贸n de dimensionalidad
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import roc_auc_score
from scipy.stats import pearsonr, spearmanr, chi2_contingency
import warnings
warnings.filterwarnings('ignore')

# Configuraci贸n de estilo para las gr谩ficas
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (14, 10)
plt.rcParams['font.size'] = 11
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 12

def load_and_prepare_data():
    """
    Carga y prepara los datos para el an谩lisis de caracter铆sticas
    """
    print("=== CARGANDO Y PREPARANDO DATOS ===")
    
    # Cargar datos
    df = pd.read_csv('hotel_booking.csv')
    
    # Limpiar datos
    df['children'] = df['children'].fillna(0)
    df['agent'] = df['agent'].fillna(0)
    df['company'] = df['company'].fillna(0)
    
    # Filtrar datos v谩lidos
    df = df[df['adr'] >= 0]
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.dropna(subset=['adr'], inplace=True)
    
    # Remover filas sin hu茅spedes
    initial_rows = df.shape[0]
    df = df[df['adults'] + df['children'] + df['babies'] > 0]
    print(f"Removidas {initial_rows - df.shape[0]} filas sin hu茅spedes")
    
    # Remover columnas de leakage
    df = df.drop(columns=['reservation_status', 'reservation_status_date'])
    
    # Reducir tama帽o del dataset para velocidad
    df = df.sample(frac=0.1, random_state=42)
    print(f"Dataset final: {df.shape[0]} filas")
    
    # Separar features y target
    X = df.drop('is_canceled', axis=1)
    y = df['is_canceled']
    
    print(f"Target distribution: {y.value_counts(normalize=True).to_dict()}")
    
    return X, y, df

def analyze_correlations(X, y):
    """
    Analiza correlaciones entre variables num茅ricas
    """
    print("\n=== ANLISIS DE CORRELACIONES ===")
    
    # Obtener variables num茅ricas
    numerical_features = X.select_dtypes(include=np.number).columns.tolist()
    X_num = X[numerical_features].copy()
    
    # Agregar target para an谩lisis
    X_num_with_target = X_num.copy()
    X_num_with_target['is_canceled'] = y
    
    # Matriz de correlaci贸n
    correlation_matrix = X_num_with_target.corr()
    
    # Crear gr谩fica de matriz de correlaci贸n
    plt.figure(figsize=(16, 12))
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                square=True, linewidths=0.5, cbar_kws={"shrink": .8})
    plt.title('Matriz de Correlaci贸n - Variables Num茅ricas', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    # An谩lisis de correlaci贸n con target
    print("\n Correlaci贸n con variable objetivo (is_canceled):")
    print("=" * 60)
    
    target_correlations = []
    for feature in numerical_features:
        corr_pearson, p_value_pearson = pearsonr(X_num[feature], y)
        corr_spearman, p_value_spearman = spearmanr(X_num[feature], y)
        
        target_correlations.append({
            'Variable': feature,
            'Correlaci贸n Pearson': corr_pearson,
            'P-value Pearson': p_value_pearson,
            'Correlaci贸n Spearman': corr_spearman,
            'P-value Spearman': p_value_spearman,
            'Significativa': p_value_pearson < 0.05
        })
    
    corr_df = pd.DataFrame(target_correlations)
    corr_df = corr_df.sort_values('Correlaci贸n Pearson', key=abs, ascending=False)
    
    print(corr_df.to_string(index=False, float_format='%.4f'))
    
    # Identificar variables altamente correlacionadas
    print("\n Variables altamente correlacionadas (|r| > 0.8):")
    print("=" * 60)
    
    high_corr_pairs = []
    for i in range(len(numerical_features)):
        for j in range(i+1, len(numerical_features)):
            feat1, feat2 = numerical_features[i], numerical_features[j]
            corr = correlation_matrix.loc[feat1, feat2]
            if abs(corr) > 0.8:
                high_corr_pairs.append({
                    'Variable 1': feat1,
                    'Variable 2': feat2,
                    'Correlaci贸n': corr
                })
    
    if high_corr_pairs:
        high_corr_df = pd.DataFrame(high_corr_pairs)
        print(high_corr_df.to_string(index=False, float_format='%.4f'))
    else:
        print("No se encontraron variables con correlaci贸n > 0.8")
    
    return corr_df, high_corr_pairs

def analyze_categorical_features(X, y):
    """
    Analiza variables categ贸ricas usando chi-cuadrado y an谩lisis de frecuencias
    """
    print("\n=== ANLISIS DE VARIABLES CATEGRICAS ===")
    
    categorical_features = X.select_dtypes(include='object').columns.tolist()
    
    if not categorical_features:
        print("No hay variables categ贸ricas para analizar")
        return pd.DataFrame()
    
    # An谩lisis de chi-cuadrado
    print("\n An谩lisis Chi-Cuadrado:")
    print("=" * 60)
    
    chi2_results = []
    for feature in categorical_features:
        # Crear tabla de contingencia
        contingency_table = pd.crosstab(X[feature], y)
        
        # Calcular chi-cuadrado
        chi2, p_value, dof, expected = chi2_contingency(contingency_table)
        
        # Calcular Cramer's V (medida de asociaci贸n)
        n = len(X)
        min_dim = min(contingency_table.shape) - 1
        cramer_v = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0
        
        chi2_results.append({
            'Variable': feature,
            'Chi2': chi2,
            'P-value': p_value,
            "Cramer's V": cramer_v,
            'Significativa': p_value < 0.05,
            'Valores 煤nicos': X[feature].nunique()
        })
    
    chi2_df = pd.DataFrame(chi2_results)
    chi2_df = chi2_df.sort_values('Chi2', ascending=False)
    
    print(chi2_df.to_string(index=False, float_format='%.4f'))
    
    # An谩lisis de frecuencias por clase
    print("\n An谩lisis de frecuencias por clase:")
    print("=" * 60)
    
    for feature in categorical_features[:5]:  # Mostrar solo las primeras 5
        print(f"\n{feature}:")
        freq_table = pd.crosstab(X[feature], y, normalize='index') * 100
        print(freq_table.round(2))
    
    return chi2_df

def analyze_discriminative_power(X, y):
    """
    Analiza la capacidad discriminativa de cada variable
    """
    print("\n=== ANLISIS DE CAPACIDAD DISCRIMINATIVA ===")
    
    # Preparar datos
    X_encoded = X.copy()
    
    # Codificar variables categ贸ricas
    le = LabelEncoder()
    categorical_features = X.select_dtypes(include='object').columns.tolist()
    for feature in categorical_features:
        X_encoded[feature] = le.fit_transform(X_encoded[feature].astype(str))
    
    # An谩lisis ANOVA F-test
    print("\n ANOVA F-test (para variables num茅ricas):")
    print("=" * 60)
    
    numerical_features = X.select_dtypes(include=np.number).columns.tolist()
    f_scores, p_values = f_classif(X_encoded[numerical_features], y)
    
    f_test_results = []
    for i, feature in enumerate(numerical_features):
        f_test_results.append({
            'Variable': feature,
            'F-score': f_scores[i],
            'P-value': p_values[i],
            'Significativa': p_values[i] < 0.05
        })
    
    f_test_df = pd.DataFrame(f_test_results)
    f_test_df = f_test_df.sort_values('F-score', ascending=False)
    
    print(f_test_df.to_string(index=False, float_format='%.4f'))
    
    # An谩lisis de informaci贸n mutua
    print("\n Informaci贸n Mutua:")
    print("=" * 60)
    
    mi_scores = mutual_info_classif(X_encoded, y, random_state=42)
    
    mi_results = []
    for i, feature in enumerate(X_encoded.columns):
        mi_results.append({
            'Variable': feature,
            'Informaci贸n Mutua': mi_scores[i]
        })
    
    mi_df = pd.DataFrame(mi_results)
    mi_df = mi_df.sort_values('Informaci贸n Mutua', ascending=False)
    
    print(mi_df.to_string(index=False, float_format='%.4f'))
    
    # Gr谩fica de capacidad discriminativa
    plt.figure(figsize=(14, 8))
    
    # Top 15 variables por informaci贸n mutua
    top_features = mi_df.head(15)
    
    plt.subplot(1, 2, 1)
    plt.barh(range(len(top_features)), top_features['Informaci贸n Mutua'])
    plt.yticks(range(len(top_features)), top_features['Variable'])
    plt.xlabel('Informaci贸n Mutua')
    plt.title('Top 15 Variables por Informaci贸n Mutua')
    plt.gca().invert_yaxis()
    
    # Top 15 variables por F-score
    top_f_features = f_test_df.head(15)
    
    plt.subplot(1, 2, 2)
    plt.barh(range(len(top_f_features)), top_f_features['F-score'])
    plt.yticks(range(len(top_f_features)), top_f_features['Variable'])
    plt.xlabel('F-score')
    plt.title('Top 15 Variables por F-score')
    plt.gca().invert_yaxis()
    
    plt.tight_layout()
    plt.show()
    
    return f_test_df, mi_df

def feature_selection_analysis(X, y):
    """
    Realiza an谩lisis de selecci贸n de caracter铆sticas
    """
    print("\n=== ANLISIS DE SELECCIN DE CARACTERSTICAS ===")
    
    # Preparar datos
    X_encoded = X.copy()
    le = LabelEncoder()
    categorical_features = X.select_dtypes(include='object').columns.tolist()
    for feature in categorical_features:
        X_encoded[feature] = le.fit_transform(X_encoded[feature].astype(str))
    
    # 1. SelectKBest con diferentes m茅todos
    print("\n SelectKBest Analysis:")
    print("=" * 60)
    
    k_values = [10, 20, 30, 40, 50]
    methods = {
        'f_classif': f_classif,
        'mutual_info_classif': mutual_info_classif
    }
    
    for method_name, method_func in methods.items():
        print(f"\n{method_name.upper()}:")
        for k in k_values:
            if k <= X_encoded.shape[1]:
                selector = SelectKBest(score_func=method_func, k=k)
                X_selected = selector.fit_transform(X_encoded, y)
                selected_features = X_encoded.columns[selector.get_support()].tolist()
                print(f"  k={k}: {len(selected_features)} caracter铆sticas seleccionadas")
    
    # 2. Random Forest Feature Importance
    print("\n Random Forest Feature Importance:")
    print("=" * 60)
    
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_encoded, y)
    
    feature_importance = pd.DataFrame({
        'Variable': X_encoded.columns,
        'Importancia': rf.feature_importances_
    }).sort_values('Importancia', ascending=False)
    
    print(feature_importance.head(20).to_string(index=False, float_format='%.4f'))
    
    # Gr谩fica de importancia de caracter铆sticas
    plt.figure(figsize=(12, 8))
    top_features = feature_importance.head(20)
    plt.barh(range(len(top_features)), top_features['Importancia'])
    plt.yticks(range(len(top_features)), top_features['Variable'])
    plt.xlabel('Importancia')
    plt.title('Top 20 Variables por Importancia (Random Forest)')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()
    
    return feature_importance

def dimensionality_reduction_analysis(X, y):
    """
    Analiza t茅cnicas de reducci贸n de dimensionalidad
    """
    print("\n=== ANLISIS DE REDUCCIN DE DIMENSIONALIDAD ===")
    
    # Preparar datos
    X_encoded = X.copy()
    le = LabelEncoder()
    categorical_features = X.select_dtypes(include='object').columns.tolist()
    for feature in categorical_features:
        X_encoded[feature] = le.fit_transform(X_encoded[feature].astype(str))
    
    # Estandarizar datos
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_encoded)
    
    # 1. An谩lisis de Componentes Principales (PCA)
    print("\n An谩lisis de Componentes Principales (PCA):")
    print("=" * 60)
    
    # PCA para diferentes n煤meros de componentes
    n_components_range = [2, 5, 10, 15, 20, 25, 30]
    explained_variances = []
    
    for n_comp in n_components_range:
        if n_comp <= X_scaled.shape[1]:
            pca = PCA(n_components=n_comp)
            pca.fit(X_scaled)
            explained_variances.append({
                'n_components': n_comp,
                'explained_variance_ratio': pca.explained_variance_ratio_.sum(),
                'cumulative_variance': pca.explained_variance_ratio_.sum()
            })
    
    pca_results = pd.DataFrame(explained_variances)
    print(pca_results.to_string(index=False, float_format='%.4f'))
    
    # Gr谩fica de varianza explicada
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(pca_results['n_components'], pca_results['explained_variance_ratio'], 'bo-')
    plt.xlabel('N煤mero de Componentes')
    plt.ylabel('Varianza Explicada')
    plt.title('Varianza Explicada vs N煤mero de Componentes')
    plt.grid(True, alpha=0.3)
    
    # PCA con 2 componentes para visualizaci贸n
    pca_2d = PCA(n_components=2)
    X_pca_2d = pca_2d.fit_transform(X_scaled)
    
    plt.subplot(1, 2, 2)
    scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y, cmap='viridis', alpha=0.6)
    plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.3f})')
    plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.3f})')
    plt.title('PCA - Visualizaci贸n 2D')
    plt.colorbar(scatter)
    
    plt.tight_layout()
    plt.show()
    
    # 2. t-SNE para visualizaci贸n
    print("\n t-SNE Analysis:")
    print("=" * 60)
    
    try:
        tsne = TSNE(n_components=2, random_state=42, perplexity=30)
        X_tsne = tsne.fit_transform(X_scaled)
        
        plt.figure(figsize=(8, 6))
        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', alpha=0.6)
        plt.xlabel('t-SNE 1')
        plt.ylabel('t-SNE 2')
        plt.title('t-SNE - Visualizaci贸n 2D')
        plt.colorbar(scatter)
        plt.tight_layout()
        plt.show()
        
        print("t-SNE completado exitosamente")
    except Exception as e:
        print(f"Error en t-SNE: {e}")
    
    return pca_results

def identify_candidate_features_for_removal(X, y):
    """
    Identifica caracter铆sticas candidatas para eliminaci贸n
    """
    print("\n=== IDENTIFICACIN DE CARACTERSTICAS CANDIDATAS PARA ELIMINACIN ===")
    
    # Preparar datos
    X_encoded = X.copy()
    le = LabelEncoder()
    categorical_features = X.select_dtypes(include='object').columns.tolist()
    for feature in categorical_features:
        X_encoded[feature] = le.fit_transform(X_encoded[feature].astype(str))
    
    # Criterios para eliminaci贸n
    candidates_for_removal = []
    
    # 1. Variables con baja informaci贸n mutua
    mi_scores = mutual_info_classif(X_encoded, y, random_state=42)
    mi_threshold = np.percentile(mi_scores, 25)  # Bottom 25%
    
    for i, feature in enumerate(X_encoded.columns):
        if mi_scores[i] < mi_threshold:
            candidates_for_removal.append({
                'Variable': feature,
                'Criterio': 'Baja informaci贸n mutua',
                'Valor': mi_scores[i],
                'Umbral': mi_threshold
            })
    
    # 2. Variables con baja correlaci贸n con target
    numerical_features = X.select_dtypes(include=np.number).columns.tolist()
    for feature in numerical_features:
        corr, p_value = pearsonr(X_encoded[feature], y)
        if abs(corr) < 0.05 and p_value > 0.05:  # Correlaci贸n baja y no significativa
            candidates_for_removal.append({
                'Variable': feature,
                'Criterio': 'Baja correlaci贸n con target',
                'Valor': corr,
                'Umbral': 0.05
            })
    
    # 3. Variables con alta correlaci贸n entre s铆 (redundantes)
    correlation_matrix = X_encoded.corr()
    for i in range(len(X_encoded.columns)):
        for j in range(i+1, len(X_encoded.columns)):
            feat1, feat2 = X_encoded.columns[i], X_encoded.columns[j]
            corr = correlation_matrix.loc[feat1, feat2]
            if abs(corr) > 0.9:  # Correlaci贸n muy alta
                # Mantener la variable con mayor informaci贸n mutua
                mi1 = mi_scores[i]
                mi2 = mi_scores[j]
                var_to_remove = feat2 if mi1 > mi2 else feat1
                candidates_for_removal.append({
                    'Variable': var_to_remove,
                    'Criterio': f'Alta correlaci贸n con {feat1 if var_to_remove == feat2 else feat2}',
                    'Valor': corr,
                    'Umbral': 0.9
                })
    
    # Crear DataFrame de candidatos
    if candidates_for_removal:
        candidates_df = pd.DataFrame(candidates_for_removal)
        candidates_df = candidates_df.drop_duplicates(subset=['Variable'])
        candidates_df = candidates_df.sort_values('Criterio')
        
        print("\n Caracter铆sticas candidatas para eliminaci贸n:")
        print("=" * 80)
        print(candidates_df.to_string(index=False, float_format='%.4f'))
        
        # Resumen por criterio
        print("\n Resumen por criterio de eliminaci贸n:")
        print("=" * 50)
        summary = candidates_df['Criterio'].value_counts()
        for criterio, count in summary.items():
            print(f"{criterio}: {count} variables")
        
        # Lista final de variables a eliminar
        variables_to_remove = candidates_df['Variable'].unique().tolist()
        print(f"\n Total de variables candidatas para eliminaci贸n: {len(variables_to_remove)}")
        print("Variables:", variables_to_remove)
        
    else:
        print("No se identificaron caracter铆sticas candidatas para eliminaci贸n")
        variables_to_remove = []
    
    return candidates_df if candidates_for_removal else pd.DataFrame(), variables_to_remove

def generate_comprehensive_report(X, y):
    """
    Genera un reporte completo del an谩lisis de caracter铆sticas
    """
    print("=== REPORTE COMPLETO DE ANLISIS DE CARACTERSTICAS ===")
    print("=" * 80)
    
    # Informaci贸n general del dataset
    print(f"\n Informaci贸n del Dataset:")
    print(f"  Total de caracter铆sticas: {X.shape[1]}")
    print(f"  Caracter铆sticas num茅ricas: {len(X.select_dtypes(include=np.number).columns)}")
    print(f"  Caracter铆sticas categ贸ricas: {len(X.select_dtypes(include='object').columns)}")
    print(f"  N煤mero de muestras: {X.shape[0]}")
    
    # An谩lisis de correlaciones
    corr_df, high_corr_pairs = analyze_correlations(X, y)
    
    # An谩lisis de variables categ贸ricas
    chi2_df = analyze_categorical_features(X, y)
    
    # An谩lisis de capacidad discriminativa
    f_test_df, mi_df = analyze_discriminative_power(X, y)
    
    # Selecci贸n de caracter铆sticas
    feature_importance = feature_selection_analysis(X, y)
    
    # Reducci贸n de dimensionalidad
    pca_results = dimensionality_reduction_analysis(X, y)
    
    # Identificaci贸n de candidatos para eliminaci贸n
    candidates_df, variables_to_remove = identify_candidate_features_for_removal(X, y)
    
    # Resumen final
    print("\n" + "="*80)
    print(" RESUMEN FINAL Y RECOMENDACIONES")
    print("="*80)
    
    print(f"\n Caracter铆sticas m谩s importantes:")
    top_features = feature_importance.head(10)
    for i, (_, row) in enumerate(top_features.iterrows(), 1):
        print(f"  {i}. {row['Variable']}: {row['Importancia']:.4f}")
    
    print(f"\n Caracter铆sticas candidatas para eliminaci贸n: {len(variables_to_remove)}")
    if variables_to_remove:
        for var in variables_to_remove:
            print(f"  - {var}")
    
    print(f"\n Recomendaciones:")
    print(f"  1. Mantener las top {min(20, len(feature_importance))} caracter铆sticas por importancia")
    print(f"  2. Considerar eliminar {len(variables_to_remove)} caracter铆sticas de baja utilidad")
    print(f"  3. Para PCA, usar {pca_results[pca_results['explained_variance_ratio'] >= 0.95]['n_components'].iloc[0]} componentes para 95% de varianza")
    print(f"  4. Evaluar el impacto de la reducci贸n en el rendimiento del modelo")
    
    return {
        'correlation_analysis': corr_df,
        'categorical_analysis': chi2_df,
        'discriminative_analysis': (f_test_df, mi_df),
        'feature_importance': feature_importance,
        'pca_results': pca_results,
        'candidates_for_removal': candidates_df,
        'variables_to_remove': variables_to_remove
    }

if __name__ == "__main__":
    # Cargar datos
    X, y, df = load_and_prepare_data()
    
    # Generar reporte completo
    results = generate_comprehensive_report(X, y) 