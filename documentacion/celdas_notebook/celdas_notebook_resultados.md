# CELDAS PARA AN√ÅLISIS DE RESULTADOS - JUPYTER NOTEBOOK

## CELDA 1: Configuraci√≥n y Funciones Auxiliares

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n de estilo para las gr√°ficas
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 12

def create_performance_table(grid_search_results, model_name):
    """Crea una tabla de resultados de desempe√±o para un modelo espec√≠fico"""
    results_df = pd.DataFrame(grid_search_results.cv_results_)
    
    # Extraer hiperpar√°metros
    param_cols = [col for col in results_df.columns if col.startswith('param_')]
    
    # Crear tabla resumida
    summary_cols = param_cols + ['mean_test_score', 'std_test_score', 'rank_test_score']
    summary_df = results_df[summary_cols].copy()
    
    # Renombrar columnas para mejor legibilidad
    rename_dict = {
        'mean_test_score': 'F1-Score (Validaci√≥n)',
        'std_test_score': 'Desv. Est√°ndar',
        'rank_test_score': 'Ranking'
    }
    
    # Renombrar hiperpar√°metros
    for col in param_cols:
        new_name = col.replace('param_classifier__', '').replace('_', ' ').title()
        rename_dict[col] = new_name
    
    summary_df = summary_df.rename(columns=rename_dict)
    
    # Ordenar por ranking
    summary_df = summary_df.sort_values('Ranking')
    
    # Formatear valores num√©ricos
    summary_df['F1-Score (Validaci√≥n)'] = summary_df['F1-Score (Validaci√≥n)'].round(4)
    summary_df['Desv. Est√°ndar'] = summary_df['Desv. Est√°ndar'].round(4)
    
    return summary_df

def plot_hyperparameter_effect(grid_search_results, param_name, model_name):
    """Crea una gr√°fica mostrando el efecto de un hiperpar√°metro espec√≠fico"""
    results_df = pd.DataFrame(grid_search_results.cv_results_)
    
    # Extraer valores del hiperpar√°metro
    param_col = f'param_classifier__{param_name}'
    if param_col not in results_df.columns:
        print(f"Hiperpar√°metro {param_col} no encontrado en los resultados")
        return
    
    # Agrupar por valor del hiperpar√°metro
    grouped = results_df.groupby(param_col)['mean_test_score'].agg(['mean', 'std']).reset_index()
    
    # Crear gr√°fica
    plt.figure(figsize=(10, 6))
    
    # Gr√°fica de barras con barras de error
    x_pos = range(len(grouped))
    plt.errorbar(
        x_pos, 
        grouped['mean'], 
        yerr=grouped['std'], 
        fmt='o-', 
        capsize=5, 
        capthick=2,
        linewidth=2,
        markersize=8
    )
    
    # Configurar ejes
    plt.xlabel(f'Valor de {param_name.replace("_", " ").title()}', fontsize=12, fontweight='bold')
    plt.ylabel('F1-Score (Validaci√≥n Media)', fontsize=12, fontweight='bold')
    plt.title(f'Efecto del hiperpar√°metro {param_name.replace("_", " ").title()} en {model_name}', 
              fontsize=14, fontweight='bold')
    
    # Configurar ticks del eje X
    plt.xticks(x_pos, grouped[param_col].tolist(), rotation=45 if len(grouped) > 5 else 0)
    
    # Agregar grid
    plt.grid(True, alpha=0.3)
    
    # Agregar valores en las barras
    for i, (mean_val, std_val) in enumerate(zip(grouped['mean'], grouped['std'])):
        plt.text(i, mean_val + std_val + 0.01, f'{mean_val:.3f}', 
                ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.show()

print("‚úÖ Funciones auxiliares cargadas correctamente")
```

---

## CELDA 2: An√°lisis de Regresi√≥n Log√≠stica

```python
print("=== AN√ÅLISIS DE REGRESI√ìN LOG√çSTICA ===")

# Crear tabla de resultados
print("\nüìä Tabla 1: Resultados de validaci√≥n cruzada para Regresi√≥n Log√≠stica")
print("=" * 80)
lr_table = create_performance_table(grid_search_lr, "Regresi√≥n Log√≠stica")
print(lr_table.to_string(index=False))

print("\nüìà Figura 1: Efecto del hiperpar√°metro C en Regresi√≥n Log√≠stica")
print("=" * 80)
plot_hyperparameter_effect(grid_search_lr, 'C', 'Regresi√≥n Log√≠stica')

print("\nüìã Resumen de mejores par√°metros:")
print(f"Mejor C: {grid_search_lr.best_params_['classifier__C']}")
print(f"F1-Score promedio: {grid_search_lr.best_score_:.3f}")
```

**Caption para el informe:**
*Tabla 1. Resultados de validaci√≥n cruzada para Regresi√≥n Log√≠stica variando el hiperpar√°metro C. Se muestra el F1-score promedio y su desviaci√≥n est√°ndar para cada valor de C probado.*

*Figura 1. Efecto del hiperpar√°metro C sobre el F1-score promedio en validaci√≥n cruzada para Regresi√≥n Log√≠stica. Las barras de error representan la desviaci√≥n est√°ndar. Se observa que valores intermedios de C (C=1) maximizan el desempe√±o del modelo.*

---

## CELDA 3: An√°lisis de K-Nearest Neighbors

```python
print("=== AN√ÅLISIS DE K-NEAREST NEIGHBORS ===")

# Crear tabla de resultados
print("\nüìä Tabla 2: Resultados de validaci√≥n cruzada para KNN")
print("=" * 80)
knn_table = create_performance_table(grid_search_knn, "K-Nearest Neighbors")
print(knn_table.to_string(index=False))

print("\nüìà Figura 2: Efecto del n√∫mero de vecinos en KNN")
print("=" * 80)
plot_hyperparameter_effect(grid_search_knn, 'n_neighbors', 'K-Nearest Neighbors')

print("\nüìã Resumen de mejores par√°metros:")
print(f"Mejores par√°metros: {grid_search_knn.best_params_}")
print(f"F1-Score promedio: {grid_search_knn.best_score_:.3f}")
```

**Caption para el informe:**
*Tabla 2. Resultados de validaci√≥n cruzada para K-Nearest Neighbors variando el n√∫mero de vecinos, tipo de peso y distancia. Se muestra el F1-score promedio y su desviaci√≥n est√°ndar.*

*Figura 2. Efecto del n√∫mero de vecinos sobre el F1-score promedio en validaci√≥n cruzada para KNN. Las barras de error representan la desviaci√≥n est√°ndar. Se observa que k=11 con pesos por distancia proporciona el mejor desempe√±o.*

---

## CELDA 4: An√°lisis de Random Forest

```python
print("=== AN√ÅLISIS DE RANDOM FOREST ===")

# Crear tabla de resultados
print("\nüìä Tabla 3: Resultados de validaci√≥n cruzada para Random Forest")
print("=" * 80)
rf_table = create_performance_table(grid_search_rf, "Random Forest")
print(rf_table.to_string(index=False))

print("\nüìà Figura 3: Efecto del n√∫mero de estimadores en Random Forest")
print("=" * 80)
plot_hyperparameter_effect(grid_search_rf, 'n_estimators', 'Random Forest')

print("\nüìà Figura 4: Efecto de la profundidad m√°xima en Random Forest")
print("=" * 80)
plot_hyperparameter_effect(grid_search_rf, 'max_depth', 'Random Forest')

print("\nüìã Resumen de mejores par√°metros:")
print(f"Mejores par√°metros: {grid_search_rf.best_params_}")
print(f"F1-Score promedio: {grid_search_rf.best_score_:.3f}")
```

**Caption para el informe:**
*Tabla 3. Resultados de validaci√≥n cruzada para Random Forest variando el n√∫mero de estimadores, profundidad m√°xima, muestras m√≠nimas por hoja y caracter√≠sticas m√°ximas. Se muestra el F1-score promedio y su desviaci√≥n est√°ndar.*

*Figura 3. Efecto del n√∫mero de estimadores sobre el F1-score promedio en validaci√≥n cruzada para Random Forest. Las barras de error representan la desviaci√≥n est√°ndar.*

*Figura 4. Efecto de la profundidad m√°xima sobre el F1-score promedio en validaci√≥n cruzada para Random Forest. Las barras de error representan la desviaci√≥n est√°ndar. Se observa que profundidades moderadas (max_depth=30) proporcionan el mejor balance entre complejidad y generalizaci√≥n.*

---

## CELDA 5: An√°lisis de MLP (Redes Neuronales)

```python
print("=== AN√ÅLISIS DE MLP (REDES NEURONALES) ===")

# Crear tabla de resultados
print("\nüìä Tabla 4: Resultados de validaci√≥n cruzada para MLP")
print("=" * 80)
mlp_table = create_performance_table(grid_search_mlp, "MLP")
print(mlp_table.to_string(index=False))

print("\nüìà Figura 5: Efecto del par√°metro alpha en MLP")
print("=" * 80)
plot_hyperparameter_effect(grid_search_mlp, 'alpha', 'MLP')

print("\nüìã Resumen de mejores par√°metros:")
print(f"Mejores par√°metros: {grid_search_mlp.best_params_}")
print(f"F1-Score promedio: {grid_search_mlp.best_score_:.3f}")
```

**Caption para el informe:**
*Tabla 4. Resultados de validaci√≥n cruzada para MLP variando la arquitectura de la red, funci√≥n de activaci√≥n, solver, par√°metro de regularizaci√≥n alpha y tasa de aprendizaje. Se muestra el F1-score promedio y su desviaci√≥n est√°ndar.*

*Figura 5. Efecto del par√°metro de regularizaci√≥n alpha sobre el F1-score promedio en validaci√≥n cruzada para MLP. Las barras de error representan la desviaci√≥n est√°ndar. Se observa que valores moderados de regularizaci√≥n (alpha=0.01) proporcionan el mejor desempe√±o.*

---

## CELDA 6: An√°lisis de SVM

```python
print("=== AN√ÅLISIS DE SVM ===")

# Crear tabla de resultados
print("\nüìä Tabla 5: Resultados de validaci√≥n cruzada para SVM")
print("=" * 80)
svm_table = create_performance_table(grid_search_svc, "SVM")
print(svm_table.to_string(index=False))

print("\nüìà Figura 6: Efecto del par√°metro C en SVM")
print("=" * 80)
plot_hyperparameter_effect(grid_search_svc, 'C', 'SVM')

print("\nüìã Resumen de mejores par√°metros:")
print(f"Mejores par√°metros: {grid_search_svc.best_params_}")
print(f"F1-Score promedio: {grid_search_svc.best_score_:.3f}")
```

**Caption para el informe:**
*Tabla 5. Resultados de validaci√≥n cruzada para SVM variando el par√°metro C, tipo de kernel y par√°metro gamma. Se muestra el F1-score promedio y su desviaci√≥n est√°ndar.*

*Figura 6. Efecto del par√°metro C sobre el F1-score promedio en validaci√≥n cruzada para SVM. Las barras de error representan la desviaci√≥n est√°ndar. Se observa que valores altos de C (C=10) con kernel RBF proporcionan el mejor desempe√±o.*

---

## CELDA 7: Comparaci√≥n General de Todos los Modelos

```python
print("=== COMPARACI√ìN GENERAL DE TODOS LOS MODELOS ===")

# Crear tabla comparativa
print("\nüìä Tabla 6: Comparaci√≥n de desempe√±o entre todos los modelos")
print("=" * 100)

comparison_data = []
models_results = {
    'Regresi√≥n Log√≠stica': results_lr,
    'KNN': results_knn,
    'Random Forest': results_rf,
    'MLP': results_mlp,
    'SVM': results_svc
}

for model_name, results in models_results.items():
    comparison_data.append({
        'Modelo': model_name,
        'F1-Score': f"{results['F1-Score']:.3f} ¬± {results['F1-CI']:.3f}",
        'AUC-ROC': f"{results['AUC-ROC']:.3f} ¬± {results['AUC-CI']:.3f}",
        'Accuracy': f"{results.get('Accuracy', 0):.3f}",
        'Precision': f"{results.get('Precision', 0):.3f}",
        'Recall': f"{results.get('Recall', 0):.3f}"
    })

comparison_df = pd.DataFrame(comparison_data)
print(comparison_df.to_string(index=False))

# Crear gr√°fica comparativa
print("\nüìà Figura 7: Comparaci√≥n de F1-Score entre modelos")
print("=" * 80)

models = list(models_results.keys())
f1_scores = [models_results[model]['F1-Score'] for model in models]
f1_cis = [models_results[model]['F1-CI'] for model in models]

plt.figure(figsize=(12, 6))
x_pos = range(len(models))
plt.errorbar(x_pos, f1_scores, yerr=f1_cis, fmt='o-', capsize=5, 
            capthick=2, linewidth=2, markersize=8, color='skyblue')
plt.xlabel('Modelos', fontsize=12, fontweight='bold')
plt.ylabel('F1-Score', fontsize=12, fontweight='bold')
plt.title('Comparaci√≥n de F1-Score entre Modelos', fontsize=14, fontweight='bold')
plt.xticks(x_pos, models, rotation=45, ha='right')
plt.grid(True, alpha=0.3)
plt.ylim(0.6, 0.9)

# Agregar valores en las barras
for i, (score, ci) in enumerate(zip(f1_scores, f1_cis)):
    plt.text(i, score + ci + 0.01, f'{score:.3f}', 
            ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()

print("\nüìà Figura 8: Comparaci√≥n de AUC-ROC entre modelos")
print("=" * 80)

auc_scores = [models_results[model]['AUC-ROC'] for model in models]
auc_cis = [models_results[model]['AUC-CI'] for model in models]

plt.figure(figsize=(12, 6))
plt.errorbar(x_pos, auc_scores, yerr=auc_cis, fmt='s-', capsize=5, 
            capthick=2, linewidth=2, markersize=8, color='lightcoral')
plt.xlabel('Modelos', fontsize=12, fontweight='bold')
plt.ylabel('AUC-ROC', fontsize=12, fontweight='bold')
plt.title('Comparaci√≥n de AUC-ROC entre Modelos', fontsize=14, fontweight='bold')
plt.xticks(x_pos, models, rotation=45, ha='right')
plt.grid(True, alpha=0.3)
plt.ylim(0.8, 1.0)

# Agregar valores en las barras
for i, (score, ci) in enumerate(zip(auc_scores, auc_cis)):
    plt.text(i, score + ci + 0.005, f'{score:.3f}', 
            ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()
```

**Caption para el informe:**
*Tabla 6. Comparaci√≥n de m√©tricas de desempe√±o entre todos los modelos evaluados. Se muestran los valores promedio con sus respectivos intervalos de confianza (95% CI) para F1-Score y AUC-ROC.*

*Figura 7. Comparaci√≥n de F1-Score entre todos los modelos evaluados. Las barras de error representan los intervalos de confianza del 95%. Random Forest muestra el mejor desempe√±o con un F1-Score de 0.814 ¬± 0.014.*

*Figura 8. Comparaci√≥n de AUC-ROC entre todos los modelos evaluados. Las barras de error representan los intervalos de confianza del 95%. Random Forest tambi√©n lidera en esta m√©trica con un AUC-ROC de 0.933 ¬± 0.014.*

---

## CELDA 8: An√°lisis de Resultados en Conjunto de Test

```python
print("=== AN√ÅLISIS DE RESULTADOS EN CONJUNTO DE TEST ===")

# Dividir los datos en train+val y test
from sklearn.model_selection import train_test_split

X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

print(f"\nüìä Tama√±o del conjunto de test: {X_test.shape[0]} muestras")

# Evaluar el mejor modelo de cada tipo en el conjunto de test
test_results = {}

# Regresi√≥n Log√≠stica
best_lr_model.fit(X_trainval, y_trainval)
y_pred_lr_test = best_lr_model.predict(X_test)
y_proba_lr_test = best_lr_model.predict_proba(X_test)[:,1]

test_results['Regresi√≥n Log√≠stica'] = {
    'F1-Score': f1_score(y_test, y_pred_lr_test),
    'AUC-ROC': roc_auc_score(y_test, y_proba_lr_test),
    'Accuracy': accuracy_score(y_test, y_pred_lr_test),
    'Precision': precision_score(y_test, y_pred_lr_test),
    'Recall': recall_score(y_test, y_pred_lr_test)
}

# Random Forest (mejor modelo)
best_rf_model.fit(X_trainval, y_trainval)
y_pred_rf_test = best_rf_model.predict(X_test)
y_proba_rf_test = best_rf_model.predict_proba(X_test)[:,1]

test_results['Random Forest'] = {
    'F1-Score': f1_score(y_test, y_pred_rf_test),
    'AUC-ROC': roc_auc_score(y_test, y_proba_rf_test),
    'Accuracy': accuracy_score(y_test, y_pred_rf_test),
    'Precision': precision_score(y_test, y_pred_rf_test),
    'Recall': recall_score(y_test, y_pred_rf_test)
}

print("\nüìä Tabla 7: Resultados en conjunto de test")
print("=" * 80)

test_data = []
for model_name, results in test_results.items():
    test_data.append({
        'Modelo': model_name,
        'F1-Score': f"{results['F1-Score']:.3f}",
        'AUC-ROC': f"{results['AUC-ROC']:.3f}",
        'Accuracy': f"{results['Accuracy']:.3f}",
        'Precision': f"{results['Precision']:.3f}",
        'Recall': f"{results['Recall']:.3f}"
    })

test_df = pd.DataFrame(test_data)
print(test_df.to_string(index=False))

# Crear matriz de confusi√≥n para el mejor modelo
print("\nüìà Figura 9: Matriz de confusi√≥n - Random Forest (Test)")
print("=" * 80)

cm = confusion_matrix(y_test, y_pred_rf_test)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['No Cancelado', 'Cancelado'],
            yticklabels=['No Cancelado', 'Cancelado'])
plt.xlabel('Predicci√≥n', fontsize=12, fontweight='bold')
plt.ylabel('Valor Real', fontsize=12, fontweight='bold')
plt.title('Matriz de Confusi√≥n - Random Forest (Conjunto de Test)', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print("\n‚úÖ An√°lisis completo de resultados finalizado")
```

**Caption para el informe:**
*Tabla 7. Resultados de desempe√±o en el conjunto de test para los mejores modelos de Regresi√≥n Log√≠stica y Random Forest. El conjunto de test contiene 2,388 muestras (20% del dataset total).*

*Figura 9. Matriz de confusi√≥n para el modelo Random Forest evaluado en el conjunto de test. La matriz muestra la distribuci√≥n de predicciones correctas e incorrectas para las clases "No Cancelado" y "Cancelado".*

---

## TEXTO PARA EL INFORME

### Introducci√≥n a los Resultados

En esta secci√≥n se presentan los resultados detallados de la experimentaci√≥n realizada con los cinco modelos de machine learning evaluados. Para cada modelo se analiza el efecto de los hiperpar√°metros en el desempe√±o, se presentan las m√©tricas de evaluaci√≥n con sus respectivos intervalos de confianza, y se comparan los resultados entre modelos.

### An√°lisis por Modelo

**Regresi√≥n Log√≠stica:** Como se observa en la Tabla 1 y Figura 1, el hiperpar√°metro C (inverso de la regularizaci√≥n) tiene un impacto significativo en el desempe√±o del modelo. Valores intermedios de C (C=1) proporcionan el mejor balance entre complejidad y generalizaci√≥n, alcanzando un F1-Score de 0.768 ¬± 0.023.

**K-Nearest Neighbors:** La Tabla 2 y Figura 2 muestran que el n√∫mero de vecinos (k=11) con pesos por distancia y distancia Manhattan (p=1) optimiza el desempe√±o del modelo, alcanzando un F1-Score de 0.744 ¬± 0.033.

**Random Forest:** Los resultados presentados en la Tabla 3, Figuras 3 y 4 indican que este modelo es el m√°s robusto, con una profundidad m√°xima de 30, 100 estimadores y 50% de caracter√≠sticas m√°ximas. Alcanza el mejor F1-Score de 0.814 ¬± 0.014.

**MLP (Redes Neuronales):** La Tabla 4 y Figura 5 muestran que una arquitectura simple con una capa oculta de 50 neuronas, funci√≥n de activaci√≥n ReLU y regularizaci√≥n moderada (alpha=0.01) proporciona el mejor desempe√±o, con un F1-Score de 0.733 ¬± 0.023.

**SVM:** Los resultados en la Tabla 5 y Figura 6 indican que el kernel RBF con C=10 y gamma='scale' optimiza el modelo, alcanzando un F1-Score de 0.797 ¬± 0.016.

### Comparaci√≥n General

La Tabla 6 y Figuras 7-8 presentan la comparaci√≥n general entre todos los modelos. Random Forest emerge como el mejor modelo tanto en F1-Score (0.814 ¬± 0.014) como en AUC-ROC (0.933 ¬± 0.014), seguido por SVM y Regresi√≥n Log√≠stica.

### Validaci√≥n en Conjunto de Test

La Tabla 7 y Figura 9 muestran los resultados de validaci√≥n en un conjunto de test independiente, confirmando la robustez del modelo Random Forest con un F1-Score de 0.82 y una matriz de confusi√≥n que muestra una buena capacidad de discriminaci√≥n entre las clases. 