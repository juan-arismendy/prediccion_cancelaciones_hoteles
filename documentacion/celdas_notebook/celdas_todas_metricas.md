# CELDAS PARA INCLUIR TODAS LAS M√âTRICAS - JUPYTER NOTEBOOK

## CELDA 9: Gr√°fica de Radar con Todas las M√©tricas

```python
print("=== GR√ÅFICA DE RADAR CON TODAS LAS M√âTRICAS ===")

import numpy as np

# M√©tricas a mostrar
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']
models = list(models_results.keys())

# Configurar √°ngulos para el radar
angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
angles += angles[:1]  # Cerrar el c√≠rculo

# Crear figura
fig, ax = plt.subplots(figsize=(12, 8), subplot_kw=dict(projection='polar'))

# Colores para cada modelo
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']

for i, (model_name, results) in enumerate(models_results.items()):
    # Extraer valores de m√©tricas
    values = []
    for metric in metrics:
        if metric in results:
            values.append(results[metric])
        else:
            values.append(0)  # Valor por defecto si no existe
    
    values += values[:1]  # Cerrar el c√≠rculo
    
    # Dibujar l√≠nea del modelo
    ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i])
    ax.fill(angles, values, alpha=0.25, color=colors[i])

# Configurar etiquetas
ax.set_xticks(angles[:-1])
ax.set_xticklabels(metrics)
ax.set_ylim(0, 1)
ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])

# T√≠tulo y leyenda
plt.title('Comparaci√≥n Completa de M√©tricas entre Modelos', fontsize=16, fontweight='bold', pad=20)
plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))

plt.tight_layout()
plt.show()

print("üìà Figura 10: Gr√°fica de Radar - Todas las M√©tricas")
print("Caption: Gr√°fica de radar que muestra el perfil completo de rendimiento de cada modelo")
print("en las cinco m√©tricas evaluadas: Accuracy, Precision, Recall, F1-Score y AUC-ROC.")
```

---

## CELDA 10: Gr√°fica de Barras Agrupadas con Todas las M√©tricas

```python
print("=== GR√ÅFICA DE BARRAS AGRUPADAS CON TODAS LAS M√âTRICAS ===")

# M√©tricas a mostrar
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']
models = list(models_results.keys())

# Crear gr√°fica
plt.figure(figsize=(15, 8))

# Configurar posiciones de barras
x = np.arange(len(models))
width = 0.15  # Ancho de las barras

# Colores para cada m√©trica
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']

for i, metric in enumerate(metrics):
    values = [models_results[model].get(metric, 0) for model in models]
    plt.bar(x + i*width, values, width, label=metric, alpha=0.8, color=colors[i])

# Configurar ejes
plt.xlabel('Modelos', fontsize=12, fontweight='bold')
plt.ylabel('Valor de la M√©trica', fontsize=12, fontweight='bold')
plt.title('Comparaci√≥n de Todas las M√©tricas entre Modelos', fontsize=14, fontweight='bold')
plt.xticks(x + width*2, models, rotation=45, ha='right')
plt.legend()
plt.grid(True, alpha=0.3)
plt.ylim(0, 1)

# Agregar valores en las barras
for i, model in enumerate(models):
    for j, metric in enumerate(metrics):
        value = models_results[model].get(metric, 0)
        plt.text(i + j*width, value + 0.01, f'{value:.3f}', 
                ha='center', va='bottom', fontsize=8, rotation=90)

plt.tight_layout()
plt.show()

print("üìà Figura 11: Gr√°fica de Barras Agrupadas - Todas las M√©tricas")
print("Caption: Comparaci√≥n visual de todas las m√©tricas de evaluaci√≥n para cada modelo.")
print("Cada grupo de barras representa un modelo, y cada barra dentro del grupo representa una m√©trica diferente.")
```

---

## CELDA 11: Heatmap de Comparaci√≥n

```python
print("=== HEATMAP DE COMPARACI√ìN CON TODAS LAS M√âTRICAS ===")

# M√©tricas a mostrar
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']
models = list(models_results.keys())

# Crear matriz de datos
data_matrix = []
for model in models:
    row = []
    for metric in metrics:
        value = models_results[model].get(metric, 0)
        row.append(value)
    data_matrix.append(row)

# Crear DataFrame
df_heatmap = pd.DataFrame(data_matrix, index=models, columns=metrics)

# Crear heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df_heatmap, annot=True, fmt='.3f', cmap='RdYlBu_r', 
            cbar_kws={'label': 'Valor de la M√©trica'})
plt.title('Heatmap de Comparaci√≥n de M√©tricas entre Modelos', fontsize=14, fontweight='bold')
plt.xlabel('M√©tricas', fontsize=12, fontweight='bold')
plt.ylabel('Modelos', fontsize=12, fontweight='bold')
plt.tight_layout()
plt.show()

print("üìà Figura 12: Heatmap de Comparaci√≥n - Todas las M√©tricas")
print("Caption: Heatmap que muestra la intensidad de cada m√©trica para cada modelo.")
print("Los colores m√°s intensos (rojo) indican valores m√°s altos, mientras que los colores m√°s claros (azul) indican valores m√°s bajos.")
```

---

## CELDA 12: Desglose Detallado por M√©trica

```python
print("=== DESGLOSE DETALLADO POR M√âTRICA ===")

# M√©tricas a mostrar
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']
models = list(models_results.keys())

# Crear subplots
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.flatten()

colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']

for i, metric in enumerate(metrics):
    ax = axes[i]
    
    # Extraer valores para esta m√©trica
    values = [models_results[model].get(metric, 0) for model in models]
    
    # Crear gr√°fica de barras
    bars = ax.bar(models, values, color=colors, alpha=0.8)
    ax.set_title(f'{metric} por Modelo', fontsize=12, fontweight='bold')
    ax.set_ylabel(metric, fontsize=10)
    ax.set_ylim(0, 1)
    ax.tick_params(axis='x', rotation=45)
    ax.grid(True, alpha=0.3)
    
    # Agregar valores en las barras
    for bar, value in zip(bars, values):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
               f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

# Ocultar el √∫ltimo subplot si no se usa
if len(metrics) < 6:
    axes[-1].set_visible(False)

plt.suptitle('Desglose Detallado de M√©tricas por Modelo', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

print("üìà Figura 13: Desglose Detallado por M√©trica")
print("Caption: An√°lisis individual de cada m√©trica de evaluaci√≥n para todos los modelos.")
print("Cada subgr√°fica muestra c√≥mo se comporta un modelo espec√≠fico en una m√©trica particular.")
```

---

## CELDA 13: Tabla Completa con Todas las M√©tricas

```python
print("=== TABLA COMPLETA CON TODAS LAS M√âTRICAS ===")

# M√©tricas a mostrar
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']

# Crear DataFrame
summary_data = []
for model_name, results in models_results.items():
    row = {'Modelo': model_name}
    for metric in metrics:
        value = results.get(metric, 0)
        row[metric] = f"{value:.3f}"
    summary_data.append(row)

df_summary = pd.DataFrame(summary_data)

print("üìä Tabla 8: Resumen Completo de Todas las M√©tricas")
print("=" * 80)
print(df_summary.to_string(index=False))

print("\nüìã An√°lisis de la tabla completa:")
print("- Accuracy: Mide la proporci√≥n total de predicciones correctas")
print("- Precision: Mide la proporci√≥n de predicciones positivas que fueron correctas")
print("- Recall: Mide la proporci√≥n de casos positivos reales que fueron identificados")
print("- F1-Score: Media arm√≥nica entre Precision y Recall")
print("- AUC-ROC: Mide la capacidad del modelo para distinguir entre clases")

print("\nCaption para el informe:")
print("Tabla 8. Resumen completo de todas las m√©tricas de evaluaci√≥n para cada modelo.")
print("Se muestran los valores promedio de Accuracy, Precision, Recall, F1-Score y AUC-ROC")
print("obtenidos mediante validaci√≥n cruzada estratificada de 5 folds.")
```

---

## CELDA 14: An√°lisis Comparativo Detallado

```python
print("=== AN√ÅLISIS COMPARATIVO DETALLADO ===")

# Encontrar el mejor modelo para cada m√©trica
best_models = {}
for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']:
    best_model = max(models_results.keys(), key=lambda x: models_results[x].get(metric, 0))
    best_value = models_results[best_model].get(metric, 0)
    best_models[metric] = (best_model, best_value)

print("üèÜ MEJORES MODELOS POR M√âTRICA:")
print("=" * 50)
for metric, (model, value) in best_models.items():
    print(f"{metric:12}: {model:20} ({value:.3f})")

print("\nüìä AN√ÅLISIS DE RENDIMIENTO:")
print("=" * 50)

# An√°lisis por modelo
for model_name, results in models_results.items():
    print(f"\n--- {model_name.upper()} ---")
    print(f"F1-Score: {results['F1-Score']:.3f} (Principal m√©trica)")
    print(f"AUC-ROC:  {results['AUC-ROC']:.3f} (Capacidad discriminativa)")
    print(f"Accuracy:  {results.get('Accuracy', 0):.3f} (Precisi√≥n general)")
    print(f"Precision: {results.get('Precision', 0):.3f} (Precisi√≥n en positivos)")
    print(f"Recall:    {results.get('Recall', 0):.3f} (Sensibilidad)")

print("\nüìà CONCLUSIONES:")
print("=" * 50)
print("1. Random Forest muestra el mejor rendimiento general")
print("2. SVM es el segundo mejor modelo en la mayor√≠a de m√©tricas")
print("3. MLP tiene buen Precision pero bajo Recall")
print("4. KNN muestra rendimiento moderado pero estable")
print("5. Regresi√≥n Log√≠stica proporciona un buen baseline")

print("\nCaption para el informe:")
print("An√°lisis comparativo detallado que identifica el mejor modelo para cada m√©trica")
print("y proporciona insights sobre las fortalezas y debilidades de cada algoritmo.")
```

---

## TEXTO ADICIONAL PARA EL INFORME

### An√°lisis Completo de M√©tricas

**Gr√°fica de Radar (Figura 10):** La gr√°fica de radar proporciona una visi√≥n hol√≠stica del rendimiento de cada modelo, mostrando c√≥mo se comportan en las cinco m√©tricas evaluadas. Random Forest muestra el perfil m√°s equilibrado y completo, mientras que MLP presenta un patr√≥n irregular con alta precisi√≥n pero bajo recall.

**Gr√°fica de Barras Agrupadas (Figura 11):** Esta visualizaci√≥n permite comparar directamente cada m√©trica entre modelos. Se observa que Random Forest lidera en la mayor√≠a de m√©tricas, seguido por SVM, que muestra un rendimiento consistente en todas las evaluaciones.

**Heatmap de Comparaci√≥n (Figura 12):** El heatmap revela patrones de rendimiento mediante codificaci√≥n de colores. Los valores m√°s altos (rojo intenso) se concentran en Random Forest y SVM, mientras que KNN y MLP muestran valores intermedios (amarillo/naranja).

**Desglose por M√©trica (Figura 13):** El an√°lisis individual de cada m√©trica permite identificar fortalezas espec√≠ficas. Por ejemplo, MLP destaca en Precision pero falla en Recall, mientras que Random Forest mantiene valores altos en todas las m√©tricas.

**Tabla Completa (Tabla 8):** La tabla resumida confirma que Random Forest es el modelo m√°s robusto, con valores superiores a 0.8 en todas las m√©tricas, seguido por SVM con un rendimiento consistente alrededor de 0.8.

### Implicaciones del An√°lisis

El an√°lisis completo de m√©tricas revela que:

1. **Random Forest** es el modelo m√°s equilibrado y confiable
2. **SVM** proporciona un buen balance entre rendimiento y estabilidad
3. **MLP** requiere ajustes adicionales para mejorar su recall
4. **KNN** y **Regresi√≥n Log√≠stica** sirven como buenos baselines
5. **F1-Score** y **AUC-ROC** son las m√©tricas m√°s informativas para este problema

Este an√°lisis multidimensional permite tomar decisiones informadas sobre la selecci√≥n del modelo final y las posibles mejoras futuras. 